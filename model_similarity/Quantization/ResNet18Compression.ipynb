{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import copy\n",
    "# torch libs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import pickle\n",
    "import utils\n",
    "from quant_utils import *\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BasicBlock(nn.Module):\n",
    "\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        #residual function\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "        )\n",
    "\n",
    "        #shortcut\n",
    "        self.shortcut = nn.Sequential()\n",
    "        self.skip_add = nn.quantized.FloatFunctional()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        #the shortcut output dimension is not the same with residual function\n",
    "        #use 1*1 convolution to match the dimension\n",
    "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        fwd = self.residual_function(x)\n",
    "        fwd = self.skip_add.add(fwd, self.shortcut(x))\n",
    "        return self.relu(fwd)\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, num_block, num_classes=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True))\n",
    "        #we use a different inputsize than the original paper\n",
    "        #so conv2_x's stride is 1\n",
    "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
    "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
    "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
    "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "\n",
    "        # we have num_block blocks per layer, the first block\n",
    "        # could be 1 or 2, other blocks would always be 1\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2_x(output)\n",
    "        output = self.conv3_x(output)\n",
    "        output = self.conv4_x(output)\n",
    "        output = self.conv5_x(output)\n",
    "        output = self.avg_pool(output)\n",
    "        output = torch.flatten(output, 1)#output.view(output.size(0), -1)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "def resnet18(num_classes=100):\n",
    "\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader, mean, std = utils.get_subtraining_dataloader_cifar10_intersect(\n",
    "    propor=1.0, \n",
    "    batch_size=128, \n",
    "    num_workers=8, \n",
    "    shuffle=True, \n",
    "    sub_idx=1)\n",
    "test_loader = utils.get_test_dataloader_cifar10(\n",
    "    mean, std, \n",
    "    batch_size=128, num_workers=8, shuffle=False, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model.\n"
     ]
    }
   ],
   "source": [
    "model = resnet18(num_classes=10)\n",
    "model.load_state_dict(\n",
    "    torch.load('/data1/checkpoint/hash/cifar10/resnet18_0.pth', map_location=device))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(\"Loaded model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_model= copy.deepcopy(model)\n",
    "model.to('cpu')\n",
    "model.eval()\n",
    "# The model has to be switched to evaluation mode before any layer fusion.\n",
    "# Otherwise the quantization will not work correctly.\n",
    "fused_model.eval()\n",
    "fused_model = torch.quantization.fuse_modules(\n",
    "    fused_model, \n",
    "    [[\"conv1.0\",\n",
    "     \"conv1.1\",\n",
    "    \"conv1.2\"]], \n",
    "    inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 !\n",
      "conv2_x !\n",
      "0 $\n",
      "1 $\n",
      "conv3_x !\n",
      "0 $\n",
      "1 $\n",
      "conv4_x !\n",
      "0 $\n",
      "1 $\n",
      "conv5_x !\n",
      "0 $\n",
      "1 $\n",
      "avg_pool !\n",
      "fc !\n"
     ]
    }
   ],
   "source": [
    "for module_name, module in fused_model.named_children():\n",
    "    print(module_name, \"!\")\n",
    "    if '_x' in module_name:\n",
    "        for basic_block_name, basic_block in module.named_children():\n",
    "            print(basic_block_name, '$')\n",
    "            for basic_subblock_name, basic_subblock in basic_block.named_children():\n",
    "                if 'residual' in basic_subblock_name:\n",
    "                    torch.quantization.fuse_modules(\n",
    "                        basic_subblock, [[\"0\", \"1\", \"2\"], [\"3\", \"4\"]], inplace=True)\n",
    "                \n",
    "                if 'shortcut' in basic_subblock_name and len(list(basic_subblock.named_children())) == 2:\n",
    "                    torch.quantization.fuse_modules(\n",
    "                        basic_subblock, [[\"0\", \"1\"]], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.quantization.observer.HistogramObserver'>, reduce_range=True), weight=functools.partial(<class 'torch.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dt/anaconda3/envs/cvnlp/lib/python3.9/site-packages/torch/quantization/observer.py:122: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuantizedNetwork(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): HistogramObserver()\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       "  (model): ResNet(\n",
       "    (conv1): Sequential(\n",
       "      (0): ConvReLU2d(\n",
       "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (activation_post_process): HistogramObserver()\n",
       "      )\n",
       "      (1): Identity()\n",
       "      (2): Identity()\n",
       "    )\n",
       "    (conv2_x): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (residual_function): Sequential(\n",
       "          (0): ConvReLU2d(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "          (3): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (4): Identity()\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (residual_function): Sequential(\n",
       "          (0): ConvReLU2d(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "          (3): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (4): Identity()\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (conv3_x): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (residual_function): Sequential(\n",
       "          (0): ConvReLU2d(\n",
       "            (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "          (3): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (4): Identity()\n",
       "        )\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(\n",
       "            64, 128, kernel_size=(1, 1), stride=(2, 2)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (1): Identity()\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (residual_function): Sequential(\n",
       "          (0): ConvReLU2d(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "          (3): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (4): Identity()\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (conv4_x): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (residual_function): Sequential(\n",
       "          (0): ConvReLU2d(\n",
       "            (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "          (3): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (4): Identity()\n",
       "        )\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(\n",
       "            128, 256, kernel_size=(1, 1), stride=(2, 2)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (1): Identity()\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (residual_function): Sequential(\n",
       "          (0): ConvReLU2d(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "          (3): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (4): Identity()\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (conv5_x): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (residual_function): Sequential(\n",
       "          (0): ConvReLU2d(\n",
       "            (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "          (3): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (4): Identity()\n",
       "        )\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(\n",
       "            256, 512, kernel_size=(1, 1), stride=(2, 2)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (1): Identity()\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (residual_function): Sequential(\n",
       "          (0): ConvReLU2d(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "          (3): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (activation_post_process): HistogramObserver()\n",
       "          )\n",
       "          (4): Identity()\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver()\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(\n",
       "      in_features=512, out_features=10, bias=True\n",
       "      (activation_post_process): HistogramObserver()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model = QuantizedNetwork(fused_model)\n",
    "quantized_model.eval()\n",
    "quantization_config = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "quantized_model.qconfig = quantization_config\n",
    "print(quantized_model.qconfig)\n",
    "torch.quantization.prepare(quantized_model, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dt/anaconda3/envs/cvnlp/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448255797/work/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27min 51s, sys: 26 s, total: 28min 17s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "calibrate_model(model=quantized_model, loader=train_loader, device='cpu')\n",
    "quantized_model = torch.quantization.convert(quantized_model, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "quantized_model.eval()\n",
    "# Print quantized model.\n",
    "# print(quantized_model)\n",
    "# Save quantized model.\n",
    "save_torchscript_model(model=quantized_model, model_dir='/data1/checkpoint/hash/cifar10/', model_filename=\"resnet18_0_quant.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model   \t Size (KB): 44776.141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44776141"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_size_of_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model   \t Size (KB): 11308.065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11308065"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_size_of_model(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:08<00:00,  9.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INT8 evaluation accuracy: 0.914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, int8_eval_accuracy = evaluate_model(model=quantized_model, test_loader=test_loader, device=device, criterion=None)\n",
    "print(\"INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:17<00:00,  4.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 evaluation accuracy: 0.916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, fp32_eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=None)\n",
    "print(\"FP32 evaluation accuracy: {:.3f}\".format(fp32_eval_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_input = torch.clamp(torch.rand(500, 3, 32, 32), 0, 1)\n",
    "rand_output  = quantized_model(rand_input).softmax(dim=1)\n",
    "import pickle\n",
    "pickle.dump(rand_output, open(\"../results/hash/cifar10/resnet18_0_quant.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5f3f0cb6f7dc525fd91e30599dc917c9059637fcae9cfadc503d008ae5db0235"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
