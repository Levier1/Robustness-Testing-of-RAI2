{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "useful-inspection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.stats as sps\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import prune\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from utils import get_network_tinyimagenet\n",
    "from conf import settings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "import quant_utils\n",
    "import copy\n",
    "import random\n",
    "seed = 0\n",
    "\n",
    "gpu = 'cuda:0'\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "concerned-extreme",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset1_mean, subset1_std = settings.TINYIMAGENET_SUBTRAIN_MEAN[0], settings.TINYIMAGENET_SUBTRAIN_STD[0]\n",
    "\n",
    "def load_model(path, norm=False, dev='cpu'):\n",
    "    vic = get_network_tinyimagenet('mobilenet_v2', False).to(dev)\n",
    "    vic.load_state_dict(torch.load(path, map_location=dev))\n",
    "    vic.eval()\n",
    "    if norm:\n",
    "        return nn.Sequential(transforms.Normalize(subset1_mean, subset1_std), vic)\n",
    "    return vic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "facial-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_prune(net, p=0.2):\n",
    "    copy_net = copy.deepcopy(net)\n",
    "    parameters_to_prune = []\n",
    "    for name, module in copy_net.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            parameters_to_prune.append((module, 'weight'))\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            parameters_to_prune.append((module, 'weight'))\n",
    "    prune.global_unstructured(\n",
    "        tuple(parameters_to_prune),\n",
    "        pruning_method=prune.L1Unstructured,\n",
    "        amount=p,\n",
    "    )\n",
    "    return copy_net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-settle",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bulgarian-motor",
   "metadata": {
    "id": "KbsVnKVm7RZo",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_path = '/data1/checkpoint/'\n",
    "related_folder_path = os.path.join(root_path, 'hash/tinyimagenet/')\n",
    "unrelated_folder_path = os.path.join(root_path, 'hash/tinyimagenet/independent')\n",
    "\n",
    "original_path = os.path.join(related_folder_path, 'mobilenet_v2_0.pth')\n",
    "quant_path = os.path.join(related_folder_path, 'mobilenet_v2_0_quant.pth')\n",
    "\n",
    "finetune_path_dict = {}dt19\n",
    "for fid in range(5):\n",
    "    folder_name = 'finetune_{}'.format(fid)\n",
    "    finetune_path_dict[folder_name] = [\n",
    "        os.path.join(related_folder_path, '{}/finetune_{}.pth'.format(folder_name, i)) for i in range(1, 11)]\n",
    "    folder_name = 'advfinetune_{}'.format(fid)\n",
    "    finetune_path_dict[folder_name] = [\n",
    "        os.path.join(related_folder_path, '{}/finetune_{}.pth'.format(folder_name, i)) for i in range(1, 11)]\n",
    "\n",
    "for fid in range(5, 10):\n",
    "    folder_name = 'finetune_{}'.format(fid)\n",
    "    finetune_path_dict[folder_name] = [\n",
    "        os.path.join(related_folder_path, '{}/finetune_{}.pth'.format(folder_name, i)) for i in range(1, 21)]\n",
    "    folder_name = 'advfinetune_{}'.format(fid)\n",
    "    finetune_path_dict[folder_name] = [\n",
    "        os.path.join(related_folder_path, '{}/finetune_{}.pth'.format(folder_name, i)) for i in range(1, 21)]\n",
    "\n",
    "unrelated_path_list = [os.path.join(unrelated_folder_path, 'model_{}.pth'.format(i)) for i in range(200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "interesting-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = True\n",
    "n = 1000\n",
    "input_shape = (n, ) + (3, 64, 64)\n",
    "# checkpoint_out = {}\n",
    "output_dict = {}\n",
    "randf = torch.rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fifteen-millennium",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:37<00:00,  4.88s/it]\n",
      "200it [01:21,  2.46it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # original net\n",
    "    original_net = load_model(original_path, norm=norm, dev=gpu)\n",
    "    output_dict['train'] = original_net(randf(input_shape, device=gpu)).softmax(dim=1).to('cpu')\n",
    "\n",
    "    # quant net\n",
    "    net = quant_utils.load_torchscript_model(quant_path, 'cpu')\n",
    "    if norm:\n",
    "        net = nn.Sequential(transforms.Normalize(subset1_mean, subset1_std), net)\n",
    "    output_dict['quant'] = net(randf(input_shape)).softmax(dim=1)\n",
    "    \n",
    "    # pruning\n",
    "    output_dict['prune'] = {}\n",
    "    for prune_p in [0.8, 0.6, 0.4, 0.2]:\n",
    "        net = global_prune(original_net, prune_p)\n",
    "        output_dict['prune'][prune_p] = net(randf(input_shape, device=gpu)).softmax(dim=1).to('cpu')\n",
    "    \n",
    "    # finetuned model\n",
    "    output_dict['finetune'] = {}\n",
    "    for name, pathlist in tqdm(finetune_path_dict.items()):\n",
    "        output_dict['finetune'][name] = []\n",
    "        for path in pathlist:\n",
    "            net = load_model(path, norm=norm, dev=gpu)\n",
    "            output_dict['finetune'][name].append(net(randf(input_shape, device=gpu)).softmax(dim=1).to('cpu'))\n",
    "\n",
    "    # finetuned model\n",
    "    output_dict['indep'] = []\n",
    "    for i, path in tqdm(enumerate(unrelated_path_list)):\n",
    "        net = load_model(path, norm=norm, dev=gpu)\n",
    "        output_dict['indep'].append(net(randf(input_shape, device=gpu)).softmax(dim=1).to('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-oxide",
   "metadata": {
    "id": "Seq98Ux78Yne"
   },
   "source": [
    "# Train autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "passive-coffee",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 50/50 [00:28<00:00,  1.75it/s, loss=2.2941999441e-06]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=200, out_features=128, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (7): ReLU()\n",
       "  (8): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (9): ReLU()\n",
       "  (10): Linear(in_features=128, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "autoencoder = nn.Sequential(\n",
    "                nn.Linear(200, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 200),\n",
    "            ).to(gpu)\n",
    "ae_dataset = torch.utils.data.TensorDataset(output_dict['train'], torch.zeros(len(output_dict['train'])).long()) \n",
    "ae_train_dataloader = torch.utils.data.DataLoader(ae_dataset, \n",
    "                                                  batch_size=8, \n",
    "                                                  shuffle=True, num_workers=8)\n",
    "n_epoch = 50\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "with tqdm(total=n_epoch, desc=\"train\") as pbar:\n",
    "    for epoch in range(n_epoch):\n",
    "        epoch_loss = 0\n",
    "        for x, y in ae_train_dataloader:\n",
    "            x = x.to(gpu)\n",
    "            outx = autoencoder(x).softmax(dim=1)\n",
    "            loss = criterion(x, outx)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * len(y)\n",
    "#         print(epoch, epoch_loss / len(ae_dataset))\n",
    "        pbar.set_postfix({'loss' : '{0:1.10e}'.format(epoch_loss / len(ae_dataset))}) \n",
    "        pbar.update(1)\n",
    "autoencoder.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-paris",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "level-prison",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 4.57e-04, 1.47e-03\n"
     ]
    }
   ],
   "source": [
    "vic_errors = []\n",
    "with torch.no_grad():\n",
    "    for i in range(1):\n",
    "        randout = original_net(randf(input_shape, device=gpu)).softmax(dim=1).cpu()\n",
    "        out = autoencoder(randout).softmax(dim=1)\n",
    "        vic_errors = torch.sum((randout - out)**2, dim=1).numpy()\n",
    "        print('{}, {:.2e}, {:.2e}'.format(i, np.mean(vic_errors), np.std(vic_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "widespread-elimination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KstestResult(statistic=0.034, pvalue=0.6101664688189142)\n",
      "4.33e-04, 1.06e-03\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = autoencoder(output_dict['train']).softmax(dim=1)\n",
    "    errors = torch.sum((output_dict['train'] - out)**2, dim=1).numpy()\n",
    "    print(sps.ks_2samp(errors, vic_errors))\n",
    "    print(\"{:.2e}, {:.2e}\".format(np.mean(errors), np.std(errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "female-preservation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KstestResult(statistic=0.067, pvalue=0.022438659451142425)\n",
      "5.51e-04, 1.78e-03\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    out = autoencoder(output_dict['quant']).softmax(dim=1)\n",
    "    errors = torch.sum((output_dict['quant'] - out)**2, dim=1).numpy()\n",
    "    print(sps.ks_2samp(errors, vic_errors))\n",
    "    print(\"{:.2e}, {:.2e}\".format(np.mean(errors), np.std(errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "quiet-shield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "6.86e-03, 7.31e-03, 9.45e-01, 0.00e+00\n",
      "0.6\n",
      "5.14e-04, 1.49e-03, 4.50e-02, 2.63e-01\n",
      "0.4\n",
      "5.18e-04, 1.35e-03, 7.70e-02, 5.31e-03\n",
      "0.2\n",
      "4.17e-04, 7.68e-04, 5.00e-02, 1.64e-01\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for k, v in output_dict['prune'].items():\n",
    "        print(k)\n",
    "        out = autoencoder(v).softmax(dim=1)\n",
    "        errors = torch.sum((v - out)**2, dim=1).numpy()\n",
    "        stats, pv = sps.ks_2samp(errors, vic_errors)\n",
    "        print(\"{:.2e}, {:.2e}, {:.2e}, {:.2e}\".format(\n",
    "            np.mean(errors), np.std(errors), stats, pv))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "composite-antibody",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_errors = {}\n",
    "indep_errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "empty-yellow",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetune_0\n",
      "0 8.30e-04, 6.05e-03, 5.60e-02, 8.69e-02\n",
      "1 5.27e-04, 1.70e-03, 5.10e-02, 1.48e-01\n",
      "2 5.83e-04, 1.92e-03, 3.40e-02, 6.10e-01\n",
      "3 6.36e-04, 1.82e-03, 6.70e-02, 2.24e-02\n",
      "4 7.42e-04, 3.70e-03, 9.30e-02, 3.48e-04\n",
      "5 9.95e-04, 3.31e-03, 1.68e-01, 9.80e-13\n",
      "6 8.43e-04, 3.17e-03, 1.55e-01, 6.76e-11\n",
      "7 8.11e-04, 3.24e-03, 1.40e-01, 5.82e-09\n",
      "8 1.41e-03, 5.46e-03, 2.65e-01, 2.83e-31\n",
      "9 1.48e-03, 6.47e-03, 2.15e-01, 1.20e-20\n",
      "advfinetune_0\n",
      "0 1.03e-03, 4.39e-03, 1.87e-01, 1.08e-15\n",
      "1 8.59e-04, 2.95e-03, 1.46e-01, 1.04e-09\n",
      "2 8.12e-04, 2.88e-03, 1.47e-01, 7.71e-10\n",
      "3 5.75e-04, 2.30e-03, 3.80e-02, 4.66e-01\n",
      "4 5.93e-04, 1.71e-03, 8.90e-02, 7.21e-04\n",
      "5 1.08e-03, 4.22e-03, 2.02e-01, 2.93e-18\n",
      "6 7.49e-04, 2.41e-03, 1.37e-01, 1.34e-08\n",
      "7 8.08e-04, 3.15e-03, 1.39e-01, 7.71e-09\n",
      "8 2.45e-03, 1.35e-02, 3.11e-01, 4.10e-43\n",
      "9 1.32e-03, 4.51e-03, 2.26e-01, 8.66e-23\n",
      "finetune_1\n",
      "0 8.57e-04, 3.69e-03, 9.90e-02, 1.10e-04\n",
      "1 6.33e-04, 2.12e-03, 5.80e-02, 6.92e-02\n",
      "2 6.25e-04, 2.46e-03, 8.20e-02, 2.39e-03\n",
      "3 4.76e-04, 1.09e-03, 3.90e-02, 4.33e-01\n",
      "4 1.01e-03, 4.92e-03, 1.31e-01, 6.77e-08\n",
      "5 6.58e-04, 2.27e-03, 8.90e-02, 7.21e-04\n",
      "6 6.98e-04, 2.28e-03, 8.70e-02, 1.03e-03\n",
      "7 1.16e-03, 3.97e-03, 2.21e-01, 8.40e-22\n",
      "8 1.16e-03, 3.38e-03, 2.03e-01, 1.94e-18\n",
      "9 1.25e-03, 4.71e-03, 1.88e-01, 7.37e-16\n",
      "advfinetune_1\n",
      "0 5.04e-04, 2.00e-03, 6.00e-02, 5.46e-02\n",
      "1 6.22e-04, 2.30e-03, 7.70e-02, 5.31e-03\n",
      "2 9.35e-04, 3.49e-03, 1.43e-01, 2.48e-09\n",
      "3 6.17e-04, 1.58e-03, 1.40e-01, 5.82e-09\n",
      "4 7.77e-04, 2.95e-03, 1.09e-01, 1.36e-05\n",
      "5 1.71e-03, 7.29e-03, 3.01e-01, 2.28e-40\n",
      "6 1.83e-03, 7.81e-03, 3.07e-01, 5.27e-42\n",
      "7 1.54e-03, 4.26e-03, 3.04e-01, 3.50e-41\n",
      "8 9.69e-04, 4.43e-03, 1.19e-01, 1.38e-06\n",
      "9 1.59e-03, 7.49e-03, 2.63e-01, 8.33e-31\n",
      "finetune_2\n",
      "0 5.52e-04, 1.79e-03, 5.40e-02, 1.08e-01\n",
      "1 7.08e-04, 2.49e-03, 1.42e-01, 3.30e-09\n",
      "2 5.81e-04, 1.47e-03, 6.50e-02, 2.92e-02\n",
      "3 4.59e-04, 1.32e-03, 2.40e-02, 9.36e-01\n",
      "4 6.24e-04, 1.96e-03, 7.10e-02, 1.29e-02\n",
      "5 8.30e-04, 2.66e-03, 1.81e-01, 1.00e-14\n",
      "6 9.13e-04, 6.88e-03, 8.10e-02, 2.82e-03\n",
      "7 8.11e-04, 2.99e-03, 1.07e-01, 2.10e-05\n",
      "8 1.07e-03, 4.58e-03, 2.05e-01, 8.49e-19\n",
      "9 6.75e-04, 2.17e-03, 1.48e-01, 5.73e-10\n",
      "advfinetune_2\n",
      "0 5.64e-04, 2.66e-03, 5.20e-02, 1.34e-01\n",
      "1 5.92e-04, 1.42e-03, 7.10e-02, 1.29e-02\n",
      "2 4.39e-04, 1.10e-03, 2.20e-02, 9.69e-01\n",
      "3 1.85e-03, 4.14e-03, 3.03e-01, 6.55e-41\n",
      "4 8.25e-04, 2.45e-03, 1.45e-01, 1.39e-09\n",
      "5 1.26e-03, 6.08e-03, 1.85e-01, 2.29e-15\n",
      "6 8.34e-04, 4.34e-03, 1.13e-01, 5.58e-06\n",
      "7 1.11e-03, 4.33e-03, 2.41e-01, 6.91e-26\n",
      "8 1.04e-03, 2.77e-03, 1.78e-01, 2.98e-14\n",
      "9 1.77e-03, 4.81e-03, 3.02e-01, 1.22e-40\n",
      "finetune_3\n",
      "0 3.63e-04, 1.44e-03, 1.13e-01, 5.58e-06\n",
      "1 4.99e-04, 1.44e-03, 3.30e-02, 6.48e-01\n",
      "2 6.65e-04, 1.65e-03, 1.36e-01, 1.77e-08\n",
      "3 5.76e-04, 2.08e-03, 5.60e-02, 8.69e-02\n",
      "4 8.12e-04, 2.69e-03, 1.20e-01, 1.08e-06\n",
      "5 1.12e-03, 5.19e-03, 1.87e-01, 1.08e-15\n",
      "6 6.32e-04, 1.25e-03, 1.47e-01, 7.71e-10\n",
      "7 9.08e-04, 2.71e-03, 2.03e-01, 1.94e-18\n",
      "8 6.55e-04, 1.60e-03, 1.55e-01, 6.76e-11\n",
      "9 8.99e-04, 2.98e-03, 1.62e-01, 7.22e-12\n",
      "advfinetune_3\n",
      "0 5.55e-04, 1.55e-03, 4.70e-02, 2.19e-01\n",
      "1 1.23e-03, 7.86e-03, 1.52e-01, 1.71e-10\n",
      "2 4.54e-04, 1.23e-03, 3.80e-02, 4.66e-01\n",
      "3 9.29e-04, 2.76e-03, 1.46e-01, 1.04e-09\n",
      "4 9.83e-04, 3.50e-03, 1.83e-01, 4.81e-15\n",
      "5 7.43e-04, 3.49e-03, 1.33e-01, 3.98e-08\n",
      "6 1.15e-03, 4.66e-03, 1.79e-01, 2.08e-14\n",
      "7 1.18e-03, 4.89e-03, 2.04e-01, 1.29e-18\n",
      "8 3.83e-04, 6.88e-04, 5.40e-02, 1.08e-01\n",
      "9 8.26e-04, 3.24e-03, 7.80e-02, 4.54e-03\n",
      "finetune_4\n",
      "0 6.91e-04, 2.72e-03, 1.01e-01, 7.34e-05\n",
      "1 5.66e-04, 1.64e-03, 9.60e-02, 1.97e-04\n",
      "2 8.42e-04, 4.75e-03, 1.04e-01, 3.96e-05\n",
      "3 5.90e-04, 2.11e-03, 5.70e-02, 7.76e-02\n",
      "4 7.37e-04, 2.32e-03, 1.22e-01, 6.67e-07\n",
      "5 5.43e-04, 1.48e-03, 7.10e-02, 1.29e-02\n",
      "6 8.89e-04, 3.88e-03, 1.49e-01, 4.25e-10\n",
      "7 1.94e-03, 6.45e-03, 2.77e-01, 3.59e-34\n",
      "8 9.94e-04, 4.26e-03, 1.40e-01, 5.82e-09\n",
      "9 1.44e-03, 5.75e-03, 2.08e-01, 2.42e-19\n",
      "advfinetune_4\n",
      "0 6.11e-04, 1.78e-03, 5.80e-02, 6.92e-02\n",
      "1 5.72e-04, 1.58e-03, 1.01e-01, 7.34e-05\n",
      "2 1.11e-03, 3.75e-03, 2.18e-01, 3.20e-21\n",
      "3 1.33e-03, 4.58e-03, 2.39e-01, 1.84e-25\n",
      "4 1.71e-03, 6.61e-03, 2.50e-01, 7.61e-28\n",
      "5 2.15e-03, 9.04e-03, 2.94e-01, 1.67e-38\n",
      "6 5.21e-04, 2.09e-03, 5.90e-02, 6.15e-02\n",
      "7 8.05e-04, 2.46e-03, 1.58e-01, 2.63e-11\n",
      "8 1.58e-03, 7.74e-03, 2.67e-01, 9.50e-32\n",
      "9 1.14e-03, 9.22e-03, 2.07e-01, 3.68e-19\n",
      "finetune_5\n",
      "0 9.40e-04, 3.01e-03, 1.41e-01, 4.39e-09\n",
      "1 4.95e-04, 1.71e-03, 5.40e-02, 1.08e-01\n",
      "2 6.53e-04, 2.18e-03, 7.10e-02, 1.29e-02\n",
      "3 8.76e-04, 3.79e-03, 1.55e-01, 6.76e-11\n",
      "4 7.28e-04, 1.90e-03, 1.39e-01, 7.71e-09\n",
      "5 1.06e-03, 3.04e-03, 2.04e-01, 1.29e-18\n",
      "6 8.87e-04, 3.13e-03, 1.44e-01, 1.86e-09\n",
      "7 6.98e-04, 2.88e-03, 8.30e-02, 2.03e-03\n",
      "8 1.04e-03, 4.09e-03, 1.81e-01, 1.00e-14\n",
      "9 7.43e-04, 2.96e-03, 1.43e-01, 2.48e-09\n",
      "10 1.70e-03, 7.19e-03, 2.81e-01, 3.63e-35\n",
      "11 5.07e-04, 1.46e-03, 5.20e-02, 1.34e-01\n",
      "12 1.12e-03, 3.69e-03, 2.29e-01, 2.16e-23\n",
      "13 8.69e-04, 2.37e-03, 2.21e-01, 8.40e-22\n",
      "14 2.03e-03, 5.80e-03, 3.50e-01, 9.66e-55\n",
      "15 1.87e-03, 7.58e-03, 2.76e-01, 6.34e-34\n",
      "16 1.37e-03, 4.94e-03, 2.63e-01, 8.33e-31\n",
      "17 1.43e-03, 6.50e-03, 2.01e-01, 4.41e-18\n",
      "18 9.56e-04, 3.07e-03, 2.04e-01, 1.29e-18\n",
      "19 1.43e-03, 4.63e-03, 2.44e-01, 1.57e-26\n",
      "advfinetune_5\n",
      "0 7.21e-04, 3.01e-03, 1.19e-01, 1.38e-06\n",
      "1 1.10e-03, 6.54e-03, 1.71e-01, 3.51e-13\n",
      "2 7.97e-04, 2.74e-03, 1.14e-01, 4.44e-06\n",
      "3 1.03e-03, 4.86e-03, 1.55e-01, 6.76e-11\n",
      "4 6.00e-04, 1.55e-03, 8.20e-02, 2.39e-03\n",
      "5 1.40e-03, 4.94e-03, 2.76e-01, 6.34e-34\n",
      "6 1.14e-03, 4.03e-03, 2.24e-01, 2.16e-22\n",
      "7 1.22e-03, 4.23e-03, 2.44e-01, 1.57e-26\n",
      "8 1.66e-03, 7.31e-03, 1.93e-01, 1.07e-16\n",
      "9 2.25e-03, 8.21e-03, 3.13e-01, 1.13e-43\n",
      "10 1.38e-03, 8.53e-03, 2.07e-01, 3.68e-19\n",
      "11 8.42e-04, 2.37e-03, 1.58e-01, 2.63e-11\n",
      "12 1.70e-03, 8.46e-03, 2.93e-01, 3.05e-38\n",
      "13 1.16e-03, 3.66e-03, 2.17e-01, 4.98e-21\n",
      "14 9.61e-04, 4.05e-03, 1.45e-01, 1.39e-09\n",
      "15 8.60e-04, 3.30e-03, 1.84e-01, 3.32e-15\n",
      "16 1.13e-03, 4.21e-03, 1.70e-01, 4.96e-13\n",
      "17 6.41e-04, 2.02e-03, 1.64e-01, 3.74e-12\n",
      "18 1.21e-03, 5.35e-03, 1.91e-01, 2.33e-16\n",
      "19 1.06e-03, 3.22e-03, 2.27e-01, 5.46e-23\n",
      "finetune_6\n",
      "0 5.60e-04, 1.92e-03, 6.20e-02, 4.28e-02\n",
      "1 7.43e-04, 2.33e-03, 1.07e-01, 2.10e-05\n",
      "2 4.75e-04, 3.91e-03, 8.40e-02, 1.72e-03\n",
      "3 7.08e-04, 2.56e-03, 5.80e-02, 6.92e-02\n",
      "4 6.99e-04, 2.58e-03, 1.50e-01, 3.14e-10\n",
      "5 1.07e-03, 5.31e-03, 1.42e-01, 3.30e-09\n",
      "6 7.80e-04, 2.31e-03, 1.21e-01, 8.51e-07\n",
      "7 1.39e-03, 3.65e-03, 2.45e-01, 9.52e-27\n",
      "8 1.12e-03, 5.38e-03, 1.79e-01, 2.08e-14\n",
      "9 7.51e-04, 2.60e-03, 1.24e-01, 4.07e-07\n",
      "10 7.21e-04, 2.21e-03, 1.02e-01, 5.98e-05\n",
      "11 1.35e-03, 5.25e-03, 2.15e-01, 1.20e-20\n",
      "12 7.56e-04, 1.86e-03, 1.21e-01, 8.51e-07\n",
      "13 9.95e-04, 2.59e-03, 1.99e-01, 9.91e-18\n",
      "14 1.59e-03, 4.61e-03, 3.13e-01, 1.13e-43\n",
      "15 1.28e-03, 3.98e-03, 2.24e-01, 2.16e-22\n",
      "16 1.35e-03, 4.10e-03, 2.55e-01, 5.77e-29\n",
      "17 1.80e-03, 6.69e-03, 2.76e-01, 6.34e-34\n",
      "18 1.90e-03, 6.29e-03, 2.89e-01, 3.35e-37\n",
      "19 1.15e-03, 3.48e-03, 2.00e-01, 6.61e-18\n",
      "advfinetune_6\n",
      "0 1.41e-03, 5.75e-03, 2.57e-01, 2.03e-29\n",
      "1 3.10e-04, 5.60e-04, 7.50e-02, 7.20e-03\n",
      "2 1.20e-03, 3.95e-03, 2.01e-01, 4.41e-18\n",
      "3 8.65e-04, 3.51e-03, 1.67e-01, 1.37e-12\n",
      "4 1.18e-03, 3.75e-03, 1.95e-01, 4.89e-17\n",
      "5 6.03e-04, 2.07e-03, 9.00e-02, 6.03e-04\n",
      "6 1.56e-03, 5.39e-03, 2.74e-01, 1.96e-33\n",
      "7 7.32e-04, 2.49e-03, 1.49e-01, 4.25e-10\n",
      "8 1.82e-03, 6.17e-03, 2.83e-01, 1.14e-35\n",
      "9 5.51e-04, 1.61e-03, 5.30e-02, 1.21e-01\n",
      "10 3.44e-03, 1.14e-02, 3.98e-01, 4.00e-71\n",
      "11 1.28e-03, 4.66e-03, 2.17e-01, 4.98e-21\n",
      "12 2.22e-03, 9.18e-03, 3.18e-01, 4.31e-45\n",
      "13 2.00e-03, 6.32e-03, 2.98e-01, 1.45e-39\n",
      "14 1.56e-03, 4.28e-03, 2.98e-01, 1.45e-39\n",
      "15 6.96e-04, 2.74e-03, 1.41e-01, 4.39e-09\n",
      "16 1.58e-03, 5.36e-03, 2.54e-01, 9.71e-29\n",
      "17 1.30e-03, 4.16e-03, 2.72e-01, 6.00e-33\n",
      "18 7.27e-04, 2.21e-03, 1.07e-01, 2.10e-05\n",
      "19 1.72e-03, 6.14e-03, 2.30e-01, 1.35e-23\n",
      "finetune_7\n",
      "0 8.25e-04, 3.80e-03, 9.80e-02, 1.33e-04\n",
      "1 5.75e-04, 4.17e-03, 2.30e-02, 9.54e-01\n",
      "2 7.70e-04, 2.23e-03, 9.80e-02, 1.33e-04\n",
      "3 1.16e-03, 6.15e-03, 1.41e-01, 4.39e-09\n",
      "4 8.33e-04, 3.05e-03, 1.48e-01, 5.73e-10\n",
      "5 9.87e-04, 3.22e-03, 2.04e-01, 1.29e-18\n",
      "6 5.81e-04, 1.66e-03, 5.70e-02, 7.76e-02\n",
      "7 9.55e-04, 3.67e-03, 1.40e-01, 5.82e-09\n",
      "8 1.27e-03, 4.02e-03, 2.41e-01, 6.91e-26\n",
      "9 7.23e-04, 2.30e-03, 1.47e-01, 7.71e-10\n",
      "10 1.22e-03, 4.16e-03, 2.01e-01, 4.41e-18\n",
      "11 7.44e-04, 1.96e-03, 1.59e-01, 1.91e-11\n",
      "12 1.52e-03, 6.30e-03, 2.41e-01, 6.91e-26\n",
      "13 7.67e-04, 2.90e-03, 9.20e-02, 4.19e-04\n",
      "14 1.25e-03, 4.31e-03, 2.41e-01, 6.91e-26\n",
      "15 7.67e-04, 3.16e-03, 1.50e-01, 3.14e-10\n",
      "16 1.02e-03, 3.48e-03, 2.19e-01, 2.05e-21\n",
      "17 1.95e-03, 6.76e-03, 2.83e-01, 1.14e-35\n",
      "18 1.44e-03, 6.24e-03, 2.50e-01, 7.61e-28\n",
      "19 1.06e-03, 3.56e-03, 1.74e-01, 1.24e-13\n",
      "advfinetune_7\n",
      "0 8.35e-04, 3.03e-03, 1.61e-01, 1.00e-11\n",
      "1 4.52e-04, 1.14e-03, 4.40e-02, 2.88e-01\n",
      "2 6.98e-04, 3.47e-03, 9.50e-02, 2.39e-04\n",
      "3 1.57e-03, 6.46e-03, 2.61e-01, 2.43e-30\n",
      "4 1.01e-03, 2.57e-03, 2.42e-01, 4.23e-26\n",
      "5 7.30e-04, 3.52e-03, 7.00e-02, 1.49e-02\n",
      "6 6.12e-04, 1.73e-03, 1.04e-01, 3.96e-05\n",
      "7 5.27e-04, 1.39e-03, 3.90e-02, 4.33e-01\n",
      "8 1.29e-03, 4.34e-03, 2.50e-01, 7.61e-28\n",
      "9 8.59e-04, 2.57e-03, 1.63e-01, 5.21e-12\n",
      "10 2.19e-03, 8.10e-03, 2.90e-01, 1.84e-37\n",
      "11 8.47e-04, 2.63e-03, 1.47e-01, 7.71e-10\n",
      "12 1.45e-03, 6.25e-03, 2.64e-01, 4.86e-31\n",
      "13 1.81e-03, 8.83e-03, 2.65e-01, 2.83e-31\n",
      "14 1.57e-03, 5.75e-03, 2.40e-01, 1.13e-25\n",
      "15 9.95e-04, 3.04e-03, 1.82e-01, 6.95e-15\n",
      "16 1.79e-03, 7.60e-03, 2.90e-01, 1.84e-37\n",
      "17 1.74e-03, 6.53e-03, 2.86e-01, 1.97e-36\n",
      "18 1.38e-03, 5.66e-03, 1.91e-01, 2.33e-16\n",
      "19 1.92e-03, 6.60e-03, 3.21e-01, 5.92e-46\n",
      "finetune_8\n",
      "0 5.72e-04, 1.60e-03, 1.04e-01, 3.96e-05\n",
      "1 6.41e-04, 2.06e-03, 1.56e-01, 4.94e-11\n",
      "2 8.64e-04, 2.89e-03, 9.80e-02, 1.33e-04\n",
      "3 8.80e-04, 2.56e-03, 1.65e-01, 2.69e-12\n",
      "4 6.86e-04, 2.25e-03, 1.12e-01, 6.99e-06\n",
      "5 5.39e-04, 2.12e-03, 4.00e-02, 4.01e-01\n",
      "6 6.52e-04, 1.50e-03, 1.21e-01, 8.51e-07\n",
      "7 1.16e-03, 3.82e-03, 2.25e-01, 1.37e-22\n",
      "8 9.25e-04, 2.72e-03, 1.79e-01, 2.08e-14\n",
      "9 1.18e-03, 3.68e-03, 2.32e-01, 5.29e-24\n",
      "10 7.93e-04, 2.92e-03, 1.30e-01, 8.80e-08\n",
      "11 1.17e-03, 4.41e-03, 1.73e-01, 1.75e-13\n",
      "12 9.55e-04, 2.73e-03, 1.87e-01, 1.08e-15\n",
      "13 1.31e-03, 4.19e-03, 2.65e-01, 2.83e-31\n",
      "14 1.18e-03, 4.27e-03, 2.22e-01, 5.36e-22\n",
      "15 2.08e-03, 6.72e-03, 3.13e-01, 1.13e-43\n",
      "16 1.47e-03, 5.04e-03, 2.40e-01, 1.13e-25\n",
      "17 9.45e-04, 3.32e-03, 1.76e-01, 6.09e-14\n",
      "18 1.18e-03, 3.42e-03, 2.20e-01, 1.32e-21\n",
      "19 1.10e-03, 2.81e-03, 1.97e-01, 2.21e-17\n",
      "advfinetune_8\n",
      "0 1.10e-03, 5.06e-03, 1.65e-01, 2.69e-12\n",
      "1 6.19e-04, 3.94e-03, 2.60e-02, 8.88e-01\n",
      "2 7.90e-04, 3.22e-03, 1.35e-01, 2.32e-08\n",
      "3 8.79e-04, 4.11e-03, 1.35e-01, 2.32e-08\n",
      "4 7.23e-04, 3.44e-03, 6.40e-02, 3.33e-02\n",
      "5 1.53e-03, 6.26e-03, 1.99e-01, 9.91e-18\n",
      "6 1.01e-03, 3.35e-03, 1.42e-01, 3.30e-09\n",
      "7 1.66e-03, 6.33e-03, 2.55e-01, 5.77e-29\n",
      "8 5.73e-04, 2.11e-03, 8.50e-02, 1.45e-03\n",
      "9 9.44e-04, 2.81e-03, 1.85e-01, 2.29e-15\n",
      "10 7.92e-04, 2.65e-03, 1.40e-01, 5.82e-09\n",
      "11 1.18e-03, 3.86e-03, 2.09e-01, 1.58e-19\n",
      "12 8.89e-04, 5.27e-03, 1.81e-01, 1.00e-14\n",
      "13 1.09e-03, 3.15e-03, 2.31e-01, 8.47e-24\n",
      "14 2.19e-03, 1.01e-02, 2.84e-01, 6.37e-36\n",
      "15 1.22e-03, 4.80e-03, 2.38e-01, 2.99e-25\n",
      "16 1.27e-03, 5.09e-03, 1.93e-01, 1.07e-16\n",
      "17 9.73e-04, 3.16e-03, 1.86e-01, 1.57e-15\n",
      "18 1.12e-03, 3.08e-03, 2.39e-01, 1.84e-25\n",
      "19 1.31e-03, 3.30e-03, 2.99e-01, 7.85e-40\n",
      "finetune_9\n",
      "0 5.86e-04, 1.85e-03, 5.90e-02, 6.15e-02\n",
      "1 5.82e-04, 2.53e-03, 3.60e-02, 5.36e-01\n",
      "2 5.48e-04, 2.32e-03, 3.70e-02, 5.01e-01\n",
      "3 5.62e-04, 1.63e-03, 8.10e-02, 2.82e-03\n",
      "4 1.25e-03, 5.33e-03, 1.92e-01, 1.58e-16\n",
      "5 8.16e-04, 2.63e-03, 1.02e-01, 5.98e-05\n",
      "6 8.01e-04, 2.95e-03, 6.30e-02, 3.78e-02\n",
      "7 1.46e-03, 5.34e-03, 2.59e-01, 7.05e-30\n",
      "8 1.20e-03, 3.64e-03, 2.22e-01, 5.36e-22\n",
      "9 7.35e-04, 2.48e-03, 1.38e-01, 1.02e-08\n",
      "10 9.41e-04, 2.63e-03, 1.97e-01, 2.21e-17\n",
      "11 6.52e-04, 1.47e-03, 1.22e-01, 6.67e-07\n",
      "12 2.08e-03, 7.30e-03, 2.81e-01, 3.63e-35\n",
      "13 1.52e-03, 1.01e-02, 1.71e-01, 3.51e-13\n",
      "14 6.73e-04, 1.94e-03, 1.05e-01, 3.21e-05\n",
      "15 1.07e-03, 3.37e-03, 1.88e-01, 7.37e-16\n",
      "16 9.56e-04, 3.07e-03, 1.51e-01, 2.32e-10\n",
      "17 8.35e-04, 2.32e-03, 1.75e-01, 8.69e-14\n",
      "18 1.28e-03, 3.36e-03, 2.41e-01, 6.91e-26\n",
      "19 1.64e-03, 5.62e-03, 3.15e-01, 3.08e-44\n",
      "advfinetune_9\n",
      "0 6.32e-04, 1.50e-03, 1.52e-01, 1.71e-10\n",
      "1 6.27e-04, 1.65e-03, 9.70e-02, 1.62e-04\n",
      "2 1.27e-03, 3.67e-03, 2.19e-01, 2.05e-21\n",
      "3 1.21e-03, 4.06e-03, 2.51e-01, 4.56e-28\n",
      "4 1.06e-03, 6.21e-03, 1.47e-01, 7.71e-10\n",
      "5 9.24e-04, 2.90e-03, 1.51e-01, 2.32e-10\n",
      "6 9.11e-04, 3.32e-03, 1.27e-01, 1.91e-07\n",
      "7 5.21e-04, 1.70e-03, 7.60e-02, 6.19e-03\n",
      "8 1.20e-03, 5.22e-03, 1.52e-01, 1.71e-10\n",
      "9 9.04e-04, 2.67e-03, 2.14e-01, 1.85e-20\n",
      "10 6.16e-04, 2.27e-03, 4.40e-02, 2.88e-01\n",
      "11 1.61e-03, 3.76e-03, 2.87e-01, 1.09e-36\n",
      "12 9.27e-04, 2.65e-03, 1.54e-01, 9.23e-11\n",
      "13 1.18e-03, 5.02e-03, 2.04e-01, 1.29e-18\n",
      "14 1.18e-03, 4.64e-03, 1.95e-01, 4.89e-17\n",
      "15 1.09e-03, 3.41e-03, 2.28e-01, 3.44e-23\n",
      "16 1.09e-03, 3.11e-03, 2.19e-01, 2.05e-21\n",
      "17 1.33e-03, 6.14e-03, 1.71e-01, 3.51e-13\n",
      "18 1.25e-03, 5.13e-03, 2.23e-01, 3.41e-22\n",
      "19 1.15e-03, 3.81e-03, 2.14e-01, 1.85e-20\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for k, vlist in output_dict['finetune'].items():\n",
    "        print(k)\n",
    "        finetune_errors[k] = []\n",
    "        for i, v in enumerate(vlist):\n",
    "            out = autoencoder(v).softmax(dim=1)\n",
    "            errors = torch.sum((v - out)**2, dim=1).numpy()\n",
    "            stats, pv = sps.ks_2samp(errors, vic_errors)\n",
    "            finetune_errors[k].append(errors)\n",
    "            print(i, \"{:.2e}, {:.2e}, {:.2e}, {:.2e}\".format(\n",
    "            np.mean(errors), np.std(errors), stats, pv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mexican-jefferson",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.98e-03, 7.81e-03, 8.55e-01, 0.00e+00\n",
      "1 9.04e-03, 9.06e-03, 9.04e-01, 0.00e+00\n",
      "2 7.54e-03, 8.00e-03, 9.03e-01, 0.00e+00\n",
      "3 1.96e-02, 1.70e-02, 9.32e-01, 0.00e+00\n",
      "4 2.93e-02, 1.83e-02, 9.83e-01, 0.00e+00\n",
      "5 2.61e-02, 2.16e-02, 9.62e-01, 0.00e+00\n",
      "6 9.55e-02, 7.30e-02, 9.81e-01, 0.00e+00\n",
      "7 8.10e-02, 7.64e-02, 9.77e-01, 0.00e+00\n",
      "8 2.01e-02, 1.87e-02, 9.26e-01, 0.00e+00\n",
      "9 4.82e-03, 8.89e-03, 7.58e-01, 7.24e-282\n",
      "10 1.18e-01, 4.63e-02, 9.97e-01, 0.00e+00\n",
      "11 1.42e-02, 1.77e-02, 9.37e-01, 0.00e+00\n",
      "12 4.35e-02, 4.12e-02, 9.72e-01, 0.00e+00\n",
      "13 6.02e-01, 1.72e-01, 1.00e+00, 0.00e+00\n",
      "14 2.03e-01, 1.17e-01, 9.94e-01, 0.00e+00\n",
      "15 4.80e-02, 2.00e-02, 9.96e-01, 0.00e+00\n",
      "16 1.56e-01, 5.67e-02, 1.00e+00, 0.00e+00\n",
      "17 9.74e-02, 6.73e-02, 9.95e-01, 0.00e+00\n",
      "18 1.45e-01, 1.06e-01, 9.92e-01, 0.00e+00\n",
      "19 1.59e-01, 8.65e-02, 9.98e-01, 0.00e+00\n",
      "20 8.61e-02, 8.20e-02, 9.85e-01, 0.00e+00\n",
      "21 1.15e-01, 7.52e-02, 9.98e-01, 0.00e+00\n",
      "22 1.04e-01, 5.84e-02, 9.92e-01, 0.00e+00\n",
      "23 1.66e-01, 7.73e-02, 9.99e-01, 0.00e+00\n",
      "24 1.84e-01, 7.55e-02, 9.99e-01, 0.00e+00\n",
      "25 5.61e-02, 4.02e-02, 9.92e-01, 0.00e+00\n",
      "26 1.33e-01, 3.77e-02, 1.00e+00, 0.00e+00\n",
      "27 3.50e-02, 4.11e-02, 9.60e-01, 0.00e+00\n",
      "28 1.47e-01, 7.21e-02, 9.98e-01, 0.00e+00\n",
      "29 6.14e-02, 4.26e-02, 9.94e-01, 0.00e+00\n",
      "30 1.44e-01, 6.70e-02, 9.97e-01, 0.00e+00\n",
      "31 7.03e-02, 4.80e-02, 9.94e-01, 0.00e+00\n",
      "32 2.42e-02, 2.83e-02, 9.75e-01, 0.00e+00\n",
      "33 4.77e-02, 4.29e-02, 9.89e-01, 0.00e+00\n",
      "34 1.32e-01, 8.20e-02, 9.95e-01, 0.00e+00\n",
      "35 1.85e-01, 1.13e-01, 9.96e-01, 0.00e+00\n",
      "36 2.12e-02, 2.33e-02, 9.44e-01, 0.00e+00\n",
      "37 2.05e-01, 7.21e-02, 1.00e+00, 0.00e+00\n",
      "38 1.23e-01, 8.71e-02, 9.95e-01, 0.00e+00\n",
      "39 1.31e-01, 3.70e-02, 1.00e+00, 0.00e+00\n",
      "40 1.76e-01, 9.76e-02, 9.98e-01, 0.00e+00\n",
      "41 1.03e-01, 4.07e-02, 9.99e-01, 0.00e+00\n",
      "42 1.37e-01, 5.87e-02, 1.00e+00, 0.00e+00\n",
      "43 4.73e-02, 3.34e-02, 9.88e-01, 0.00e+00\n",
      "44 9.73e-02, 3.15e-02, 9.98e-01, 0.00e+00\n",
      "45 4.76e-02, 3.18e-02, 9.95e-01, 0.00e+00\n",
      "46 2.33e-01, 1.07e-01, 9.99e-01, 0.00e+00\n",
      "47 2.80e-02, 2.19e-02, 9.73e-01, 0.00e+00\n",
      "48 1.19e-01, 6.19e-02, 9.97e-01, 0.00e+00\n",
      "49 1.05e-01, 5.75e-02, 9.95e-01, 0.00e+00\n",
      "50 8.20e-02, 6.93e-02, 9.82e-01, 0.00e+00\n",
      "51 3.14e-02, 2.90e-02, 9.66e-01, 0.00e+00\n",
      "52 1.65e-01, 8.17e-02, 9.99e-01, 0.00e+00\n",
      "53 1.19e-01, 5.42e-02, 9.99e-01, 0.00e+00\n",
      "54 4.32e-02, 4.08e-02, 9.83e-01, 0.00e+00\n",
      "55 3.41e-01, 1.25e-01, 9.99e-01, 0.00e+00\n",
      "56 5.55e-02, 3.44e-02, 9.95e-01, 0.00e+00\n",
      "57 1.85e-02, 1.63e-02, 9.51e-01, 0.00e+00\n",
      "58 2.18e-01, 5.94e-02, 1.00e+00, 0.00e+00\n",
      "59 1.36e-01, 9.00e-02, 9.98e-01, 0.00e+00\n",
      "60 1.77e-01, 9.29e-02, 9.98e-01, 0.00e+00\n",
      "61 5.32e-01, 1.06e-01, 1.00e+00, 0.00e+00\n",
      "62 9.72e-02, 5.79e-02, 9.95e-01, 0.00e+00\n",
      "63 5.29e-02, 2.93e-02, 9.93e-01, 0.00e+00\n",
      "64 1.37e-01, 5.05e-02, 9.99e-01, 0.00e+00\n",
      "65 1.45e-01, 8.58e-02, 9.96e-01, 0.00e+00\n",
      "66 1.87e-02, 1.66e-02, 9.46e-01, 0.00e+00\n",
      "67 6.46e-02, 3.15e-02, 9.93e-01, 0.00e+00\n",
      "68 2.04e-01, 7.93e-02, 1.00e+00, 0.00e+00\n",
      "69 2.13e-01, 9.07e-02, 1.00e+00, 0.00e+00\n",
      "70 1.08e-01, 3.72e-02, 9.98e-01, 0.00e+00\n",
      "71 1.10e-01, 4.82e-02, 1.00e+00, 0.00e+00\n",
      "72 1.52e-01, 7.89e-02, 9.98e-01, 0.00e+00\n",
      "73 1.63e-01, 5.05e-02, 1.00e+00, 0.00e+00\n",
      "74 1.63e-01, 6.15e-02, 1.00e+00, 0.00e+00\n",
      "75 6.52e-02, 4.12e-02, 9.96e-01, 0.00e+00\n",
      "76 6.79e-02, 3.32e-02, 9.96e-01, 0.00e+00\n",
      "77 1.24e-01, 5.14e-02, 1.00e+00, 0.00e+00\n",
      "78 1.13e-01, 5.24e-02, 9.97e-01, 0.00e+00\n",
      "79 2.25e-02, 1.32e-02, 9.89e-01, 0.00e+00\n",
      "80 6.86e-02, 3.67e-02, 9.93e-01, 0.00e+00\n",
      "81 9.25e-02, 2.44e-02, 9.98e-01, 0.00e+00\n",
      "82 8.67e-02, 7.39e-02, 9.89e-01, 0.00e+00\n",
      "83 4.39e-01, 1.30e-01, 1.00e+00, 0.00e+00\n",
      "84 1.02e-01, 3.85e-02, 9.98e-01, 0.00e+00\n",
      "85 9.70e-02, 4.53e-02, 9.96e-01, 0.00e+00\n",
      "86 8.79e-02, 7.53e-02, 9.80e-01, 0.00e+00\n",
      "87 2.70e-01, 1.19e-01, 9.99e-01, 0.00e+00\n",
      "88 4.24e-01, 1.59e-01, 1.00e+00, 0.00e+00\n",
      "89 1.22e-01, 1.05e-01, 9.87e-01, 0.00e+00\n",
      "90 1.03e-01, 7.30e-02, 9.95e-01, 0.00e+00\n",
      "91 4.61e-01, 1.16e-01, 1.00e+00, 0.00e+00\n",
      "92 9.01e-02, 4.75e-02, 9.98e-01, 0.00e+00\n",
      "93 7.25e-01, 2.30e-01, 1.00e+00, 0.00e+00\n",
      "94 1.39e-01, 4.81e-02, 9.97e-01, 0.00e+00\n",
      "95 8.86e-02, 4.64e-02, 9.96e-01, 0.00e+00\n",
      "96 5.29e-01, 1.20e-01, 1.00e+00, 0.00e+00\n",
      "97 3.82e-02, 4.14e-02, 9.64e-01, 0.00e+00\n",
      "98 1.59e-01, 1.01e-01, 9.95e-01, 0.00e+00\n",
      "99 8.65e-02, 5.39e-02, 9.95e-01, 0.00e+00\n",
      "100 2.59e-01, 1.26e-01, 9.98e-01, 0.00e+00\n",
      "101 1.01e-01, 4.19e-02, 9.99e-01, 0.00e+00\n",
      "102 4.11e-02, 3.50e-02, 9.91e-01, 0.00e+00\n",
      "103 2.05e-01, 5.91e-02, 1.00e+00, 0.00e+00\n",
      "104 1.14e-01, 5.86e-02, 9.95e-01, 0.00e+00\n",
      "105 2.16e-01, 1.09e-01, 1.00e+00, 0.00e+00\n",
      "106 6.10e-01, 9.40e-02, 1.00e+00, 0.00e+00\n",
      "107 6.12e-02, 3.39e-02, 9.93e-01, 0.00e+00\n",
      "108 4.27e-01, 1.66e-01, 1.00e+00, 0.00e+00\n",
      "109 1.63e-01, 5.69e-02, 1.00e+00, 0.00e+00\n",
      "110 6.62e-02, 3.59e-02, 9.98e-01, 0.00e+00\n",
      "111 1.35e-01, 1.05e-01, 9.94e-01, 0.00e+00\n",
      "112 5.50e-01, 1.11e-01, 1.00e+00, 0.00e+00\n",
      "113 7.84e-02, 3.57e-02, 9.98e-01, 0.00e+00\n",
      "114 8.37e-02, 5.38e-02, 9.93e-01, 0.00e+00\n",
      "115 5.81e-02, 3.06e-02, 9.95e-01, 0.00e+00\n",
      "116 4.42e-02, 2.33e-02, 9.91e-01, 0.00e+00\n",
      "117 2.58e-01, 1.31e-01, 9.98e-01, 0.00e+00\n",
      "118 1.53e-01, 4.77e-02, 9.98e-01, 0.00e+00\n",
      "119 5.27e-02, 4.63e-02, 9.85e-01, 0.00e+00\n",
      "120 1.55e-01, 4.58e-02, 9.99e-01, 0.00e+00\n",
      "121 6.37e-02, 3.29e-02, 9.98e-01, 0.00e+00\n",
      "122 1.97e-01, 8.94e-02, 9.98e-01, 0.00e+00\n",
      "123 1.29e-01, 3.30e-02, 1.00e+00, 0.00e+00\n",
      "124 1.52e-01, 4.41e-02, 1.00e+00, 0.00e+00\n",
      "125 1.76e-01, 5.18e-02, 1.00e+00, 0.00e+00\n",
      "126 2.25e-01, 8.15e-02, 1.00e+00, 0.00e+00\n",
      "127 1.88e-01, 7.01e-02, 9.99e-01, 0.00e+00\n",
      "128 1.51e-01, 8.67e-02, 9.96e-01, 0.00e+00\n",
      "129 2.52e-02, 1.95e-02, 9.78e-01, 0.00e+00\n",
      "130 1.70e-01, 2.57e-02, 1.00e+00, 0.00e+00\n",
      "131 2.07e-01, 8.38e-02, 1.00e+00, 0.00e+00\n",
      "132 6.90e-02, 4.71e-02, 9.95e-01, 0.00e+00\n",
      "133 4.84e-02, 3.51e-02, 9.91e-01, 0.00e+00\n",
      "134 4.47e-02, 4.26e-02, 9.80e-01, 0.00e+00\n",
      "135 7.61e-02, 2.62e-02, 9.98e-01, 0.00e+00\n",
      "136 7.14e-02, 5.10e-02, 9.92e-01, 0.00e+00\n",
      "137 5.73e-02, 2.62e-02, 9.95e-01, 0.00e+00\n",
      "138 6.30e-02, 4.65e-02, 9.79e-01, 0.00e+00\n",
      "139 9.89e-02, 8.84e-02, 9.95e-01, 0.00e+00\n",
      "140 4.26e-02, 3.86e-02, 9.78e-01, 0.00e+00\n",
      "141 1.55e-01, 7.95e-02, 9.96e-01, 0.00e+00\n",
      "142 9.69e-02, 4.38e-02, 9.97e-01, 0.00e+00\n",
      "143 3.44e-02, 3.63e-02, 9.49e-01, 0.00e+00\n",
      "144 1.05e-01, 4.99e-02, 9.97e-01, 0.00e+00\n",
      "145 9.78e-02, 6.43e-02, 9.93e-01, 0.00e+00\n",
      "146 1.39e-01, 4.96e-02, 1.00e+00, 0.00e+00\n",
      "147 1.51e-01, 4.76e-02, 9.98e-01, 0.00e+00\n",
      "148 1.58e-02, 9.13e-03, 9.74e-01, 0.00e+00\n",
      "149 6.25e-02, 3.06e-02, 9.96e-01, 0.00e+00\n",
      "150 9.70e-02, 3.66e-02, 9.97e-01, 0.00e+00\n",
      "151 1.80e-01, 4.36e-02, 1.00e+00, 0.00e+00\n",
      "152 3.57e-02, 3.43e-02, 9.52e-01, 0.00e+00\n",
      "153 8.82e-02, 6.54e-02, 9.91e-01, 0.00e+00\n",
      "154 6.28e-02, 5.75e-02, 9.91e-01, 0.00e+00\n",
      "155 1.13e-01, 5.29e-02, 9.96e-01, 0.00e+00\n",
      "156 3.02e-02, 2.39e-02, 9.70e-01, 0.00e+00\n",
      "157 4.49e-01, 1.67e-01, 1.00e+00, 0.00e+00\n",
      "158 1.83e-01, 5.18e-02, 1.00e+00, 0.00e+00\n",
      "159 4.46e-01, 1.35e-01, 1.00e+00, 0.00e+00\n",
      "160 1.96e-01, 5.78e-02, 1.00e+00, 0.00e+00\n",
      "161 5.91e-02, 4.15e-02, 9.95e-01, 0.00e+00\n",
      "162 4.33e-02, 2.69e-02, 9.89e-01, 0.00e+00\n",
      "163 2.62e-02, 1.75e-02, 9.91e-01, 0.00e+00\n",
      "164 3.72e-01, 1.24e-01, 1.00e+00, 0.00e+00\n",
      "165 7.14e-02, 5.71e-02, 9.87e-01, 0.00e+00\n",
      "166 2.61e-01, 1.10e-01, 9.99e-01, 0.00e+00\n",
      "167 1.68e-01, 7.01e-02, 9.98e-01, 0.00e+00\n",
      "168 6.85e-02, 3.86e-02, 9.95e-01, 0.00e+00\n",
      "169 2.34e-01, 6.95e-02, 1.00e+00, 0.00e+00\n",
      "170 2.05e-01, 7.03e-02, 9.99e-01, 0.00e+00\n",
      "171 2.49e-01, 7.83e-02, 1.00e+00, 0.00e+00\n",
      "172 7.27e-02, 2.66e-02, 9.97e-01, 0.00e+00\n",
      "173 1.66e-01, 1.19e-01, 9.92e-01, 0.00e+00\n",
      "174 1.40e-01, 4.76e-02, 1.00e+00, 0.00e+00\n",
      "175 4.39e-02, 4.11e-02, 9.69e-01, 0.00e+00\n",
      "176 8.79e-03, 7.90e-03, 9.36e-01, 0.00e+00\n",
      "177 8.87e-02, 5.32e-02, 9.97e-01, 0.00e+00\n",
      "178 6.28e-02, 3.84e-02, 9.85e-01, 0.00e+00\n",
      "179 1.71e-01, 7.02e-02, 9.99e-01, 0.00e+00\n",
      "180 7.10e-02, 4.75e-02, 9.94e-01, 0.00e+00\n",
      "181 2.80e-01, 7.78e-02, 1.00e+00, 0.00e+00\n",
      "182 1.99e-01, 8.20e-02, 1.00e+00, 0.00e+00\n",
      "183 1.32e-01, 4.69e-02, 1.00e+00, 0.00e+00\n",
      "184 1.31e-01, 6.28e-02, 1.00e+00, 0.00e+00\n",
      "185 5.13e-02, 4.28e-02, 9.85e-01, 0.00e+00\n",
      "186 8.15e-02, 4.05e-02, 9.96e-01, 0.00e+00\n",
      "187 4.14e-01, 1.67e-01, 9.97e-01, 0.00e+00\n",
      "188 4.32e-01, 1.57e-01, 1.00e+00, 0.00e+00\n",
      "189 1.31e-01, 8.38e-02, 9.94e-01, 0.00e+00\n",
      "190 8.03e-02, 3.67e-02, 9.98e-01, 0.00e+00\n",
      "191 2.21e-01, 1.19e-01, 9.91e-01, 0.00e+00\n",
      "192 3.25e-02, 1.90e-02, 9.90e-01, 0.00e+00\n",
      "193 5.91e-02, 2.61e-02, 9.98e-01, 0.00e+00\n",
      "194 2.04e-02, 2.01e-02, 9.36e-01, 0.00e+00\n",
      "195 2.39e-02, 2.34e-02, 9.56e-01, 0.00e+00\n",
      "196 9.76e-03, 1.38e-02, 9.02e-01, 0.00e+00\n",
      "197 3.07e-02, 2.87e-02, 9.47e-01, 0.00e+00\n",
      "198 8.37e-02, 3.47e-02, 9.99e-01, 0.00e+00\n",
      "199 8.64e-02, 5.17e-02, 9.93e-01, 0.00e+00\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i, v in enumerate(output_dict['indep']):\n",
    "        out = autoencoder(v).softmax(dim=1)\n",
    "        errors = torch.sum((v - out)**2, dim=1).numpy()\n",
    "        indep_errors.append(errors)\n",
    "        stats, pv = sps.ks_2samp(errors, vic_errors)\n",
    "        print(i, \"{:.2e}, {:.2e}, {:.2e}, {:.2e}\".format(\n",
    "            np.mean(errors), np.std(errors), stats, pv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-filing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "rough-proposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(autoencoder.state_dict(), '../results/hash/tinyimagenet/autoencoder.pt')\n",
    "\n",
    "pickle.dump((output_dict, finetune_errors, indep_errors), open(\"../results/hash/tinyimagenet/output_dict.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cultural-modification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00245189\n"
     ]
    }
   ],
   "source": [
    "finetune_mean_errors = []\n",
    "for k, vlist in finetune_errors.items():\n",
    "    if int(k.split('_')[-1])<5:\n",
    "        for v in vlist:\n",
    "            finetune_mean_errors.append(np.mean(v))\n",
    "print(np.max(finetune_mean_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "hidden-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_finetune_mean_errors = []\n",
    "for k, vlist in finetune_errors.items():\n",
    "    if int(k.split('_')[-1])>4:\n",
    "        for v in vlist[:20]:\n",
    "            \n",
    "            test_finetune_mean_errors.append(np.mean(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "three-gospel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "test_finetune_mean_errors = np.array(test_finetune_mean_errors)\n",
    "print(len(test_finetune_mean_errors[test_finetune_mean_errors>np.max(finetune_mean_errors)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "olympic-mongolia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "test_indep_mean_errors = []\n",
    "for v in indep_errors[:200]:\n",
    "    test_indep_mean_errors.append(np.mean(v))\n",
    "test_indep_mean_errors = np.array(test_indep_mean_errors)\n",
    "print(len(test_indep_mean_errors[test_indep_mean_errors<np.max(finetune_mean_errors)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-aquatic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-japanese",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-richmond",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-discount",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "studied-guatemala",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import compute_accuracy, get_test_dataloader_tinyimagenet\n",
    "# test_loader = get_test_dataloader_tinyimagenet((0, 0, 0), (1, 1, 1))\n",
    "# acc, _, _ = compute_accuracy(original_net, test_loader, 'cpu')\n",
    "# accquant, _, _ = compute_accuracy(quant_net, test_loader, 'cpu')\n",
    "# acc20, _, _ = compute_accuracy(pruned_model_2, test_loader, 'cpu')\n",
    "# acc40, _, _ = compute_accuracy(pruned_model_4, test_loader, 'cpu')\n",
    "# acc60, _, _ = compute_accuracy(pruned_model_6, test_loader, 'cpu')\n",
    "# acc80, _, _ = compute_accuracy(pruned_model_8, test_loader, 'cpu')\n",
    "# print(acc, accquant, acc20, acc40, acc60, acc80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-liverpool",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-heath",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5f3f0cb6f7dc525fd91e30599dc917c9059637fcae9cfadc503d008ae5db0235"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
