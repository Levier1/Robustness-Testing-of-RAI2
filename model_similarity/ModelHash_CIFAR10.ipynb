{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "systematic-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.stats as sps\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import prune\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from utils import get_network\n",
    "from conf import settings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "import quant_utils\n",
    "import copy\n",
    "import random\n",
    "seed = 0\n",
    "\n",
    "device = 'cpu'\n",
    "gpu = 'cuda:1'\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "verified-outdoors",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset1_mean, subset1_std = settings.CIFAR10_SUBTRAIN_MEAN[0], settings.CIFAR10_SUBTRAIN_STD[0]\n",
    "\n",
    "def load_model(path, norm=False, dev='cpu'):\n",
    "    vic = get_network('resnet18', False, num_classes=10).to(dev)\n",
    "    vic.load_state_dict(torch.load(path, map_location=dev))\n",
    "    vic.eval()\n",
    "    if norm:\n",
    "        return nn.Sequential(transforms.Normalize(subset1_mean, subset1_std), vic)\n",
    "    return vic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "impressive-detective",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_prune(net, p=0.2):\n",
    "    copy_net = copy.deepcopy(net)\n",
    "    parameters_to_prune = []\n",
    "    for name, module in copy_net.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            parameters_to_prune.append((module, 'weight'))\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            parameters_to_prune.append((module, 'weight'))\n",
    "    prune.global_unstructured(\n",
    "        tuple(parameters_to_prune),\n",
    "        pruning_method=prune.L1Unstructured,\n",
    "        amount=p,\n",
    "    )\n",
    "    return copy_net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-transmission",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "finished-satellite",
   "metadata": {
    "id": "KbsVnKVm7RZo",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_path = '/data1/checkpoint/'\n",
    "related_folder_path = os.path.join(root_path, 'hash/cifar10/')\n",
    "unrelated_folder_path = os.path.join(root_path, 'hash/cifar10/independent')\n",
    "\n",
    "original_path = os.path.join(related_folder_path, 'resnet18_0.pth')\n",
    "quant_path = os.path.join(related_folder_path, 'resnet18_0_quant.pth')\n",
    "\n",
    "finetune_path_dict = {}\n",
    "for fid in range(5):\n",
    "    folder_name = 'finetune_{}'.format(fid)\n",
    "    finetune_path_dict[folder_name] = [\n",
    "        os.path.join(related_folder_path, '{}/finetune_{}.pth'.format(folder_name, i)) for i in range(1, 11)]\n",
    "    folder_name = 'advfinetune_{}'.format(fid)\n",
    "    finetune_path_dict[folder_name] = [\n",
    "        os.path.join(related_folder_path, '{}/finetune_{}.pth'.format(folder_name, i)) for i in range(1, 11)]\n",
    "\n",
    "for fid in range(5, 10):\n",
    "    folder_name = 'finetune_{}'.format(fid)\n",
    "    finetune_path_dict[folder_name] = [\n",
    "        os.path.join(related_folder_path, '{}/finetune_{}.pth'.format(folder_name, i)) for i in range(1, 21)]\n",
    "    folder_name = 'advfinetune_{}'.format(fid)\n",
    "    finetune_path_dict[folder_name] = [\n",
    "        os.path.join(related_folder_path, '{}/finetune_{}.pth'.format(folder_name, i)) for i in range(1, 21)]\n",
    "\n",
    "unrelated_path_list = [os.path.join(unrelated_folder_path, 'model_{}.pth'.format(i)) for i in range(200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "medium-movement",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "norm = True\n",
    "n = 1000\n",
    "input_shape = (n, ) + (3, 32, 32)\n",
    "# checkpoint_out = {}\n",
    "output_dict = {}\n",
    "randf = torch.rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "literary-packet",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:39<00:00,  1.98s/it]\n",
      "200it [01:05,  3.05it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # original net\n",
    "    original_net = load_model(original_path, norm=norm, dev=gpu)\n",
    "    output_dict['train'] = original_net(randf(input_shape, device=gpu)).softmax(dim=1).to('cpu')\n",
    "\n",
    "    # quant net\n",
    "    net = quant_utils.load_torchscript_model(quant_path, 'cpu')\n",
    "    if norm:\n",
    "        net = nn.Sequential(transforms.Normalize(subset1_mean, subset1_std), net)\n",
    "    output_dict['quant'] = net(randf(input_shape)).softmax(dim=1)\n",
    "    \n",
    "    # pruning\n",
    "    output_dict['prune'] = {}\n",
    "    for prune_p in [0.8, 0.6, 0.4, 0.2]:\n",
    "        net = global_prune(original_net, prune_p)\n",
    "        output_dict['prune'][prune_p] = net(randf(input_shape, device=gpu)).softmax(dim=1).to('cpu')\n",
    "    \n",
    "    # finetuned model\n",
    "    output_dict['finetune'] = {}\n",
    "    for name, pathlist in tqdm(finetune_path_dict.items()):\n",
    "        output_dict['finetune'][name] = []\n",
    "        for path in pathlist:\n",
    "            net = load_model(path, norm=norm, dev=gpu)\n",
    "            output_dict['finetune'][name].append(net(randf(input_shape, device=gpu)).softmax(dim=1).to('cpu'))\n",
    "\n",
    "    # finetuned model\n",
    "    output_dict['indep'] = []\n",
    "    for i, path in tqdm(enumerate(unrelated_path_list)):\n",
    "        net = load_model(path, norm=norm, dev=gpu)\n",
    "        output_dict['indep'].append(net(randf(input_shape, device=gpu)).softmax(dim=1).to('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-observation",
   "metadata": {
    "id": "Seq98Ux78Yne"
   },
   "source": [
    "# Train autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "lesser-highland",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 50/50 [00:33<00:00,  1.51it/s, loss=2.9479244185e-06]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "autoencoder = nn.Sequential(\n",
    "                nn.Linear(10, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 10),\n",
    "            ).to(gpu)\n",
    "ae_dataset = torch.utils.data.TensorDataset(output_dict['train'], torch.zeros(len(output_dict['train'])).long()) \n",
    "ae_train_dataloader = torch.utils.data.DataLoader(ae_dataset, \n",
    "                                                  batch_size=8, \n",
    "                                                  shuffle=True, num_workers=8)\n",
    "n_epoch = 50\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=2e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "with tqdm(total=n_epoch, desc=\"train\") as pbar:\n",
    "    for epoch in range(n_epoch):\n",
    "        epoch_loss = 0\n",
    "        for x, y in ae_train_dataloader:\n",
    "            x = x.to(gpu)\n",
    "            outx = autoencoder(x).softmax(dim=1)\n",
    "            loss = criterion(x, outx)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * len(y)\n",
    "#         print(epoch, epoch_loss / len(ae_dataset))\n",
    "        pbar.set_postfix({'loss' : '{0:1.10e}'.format(epoch_loss / len(ae_dataset))}) \n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fifty-improvement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=10, out_features=64, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "material-wyoming",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 8.08e-05, 1.02e-03\n"
     ]
    }
   ],
   "source": [
    "vic_errors = []\n",
    "with torch.no_grad():\n",
    "    for i in range(1):\n",
    "        randout = original_net(randf(input_shape, device=gpu)).softmax(dim=1).cpu()\n",
    "        out = autoencoder(randout).softmax(dim=1)\n",
    "        vic_errors = torch.sum((randout - out)**2, dim=1).numpy()\n",
    "        print('{}, {:.2e}, {:.2e}'.format(i, np.mean(vic_errors), np.std(vic_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "periodic-strain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KstestResult(statistic=0.037, pvalue=0.5005673707894058)\n",
      "2.56e-05, 6.29e-05\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = autoencoder(output_dict['train']).softmax(dim=1)\n",
    "    errors = torch.sum((output_dict['train'] - out)**2, dim=1).numpy()\n",
    "    print(sps.ks_2samp(errors, vic_errors))\n",
    "    print(\"{:.2e}, {:.2e}\".format(np.mean(errors), np.std(errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "prescription-sperm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KstestResult(statistic=0.035, pvalue=0.5728904395829821)\n",
      "3.71e-04, 7.13e-03\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    out = autoencoder(output_dict['quant']).softmax(dim=1)\n",
    "    errors = torch.sum((output_dict['quant'] - out)**2, dim=1).numpy()\n",
    "    print(sps.ks_2samp(errors, vic_errors))\n",
    "    print(\"{:.2e}, {:.2e}\".format(np.mean(errors), np.std(errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "timely-richmond",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "1.48e-01, 3.71e-02, 1.00e+00, 0.00e+00\n",
      "0.6\n",
      "1.10e-03, 5.43e-03, 4.83e-01, 4.81e-106\n",
      "0.4\n",
      "1.14e-04, 1.16e-03, 4.20e-02, 3.41e-01\n",
      "0.2\n",
      "2.88e-04, 6.15e-03, 3.10e-02, 7.23e-01\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for k, v in output_dict['prune'].items():\n",
    "        print(k)\n",
    "        out = autoencoder(v).softmax(dim=1)\n",
    "        errors = torch.sum((v - out)**2, dim=1).numpy()\n",
    "        stats, pv = sps.ks_2samp(errors, vic_errors)\n",
    "        print(\"{:.2e}, {:.2e}, {:.2e}, {:.2e}\".format(\n",
    "            np.mean(errors), np.std(errors), stats, pv))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "mental-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_errors = {}\n",
    "indep_errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "wicked-relief",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetune_0\n",
      "0 2.44e-05, 1.10e-04, 4.50e-02, 2.63e-01\n",
      "1 5.27e-05, 8.20e-04, 3.90e-02, 4.33e-01\n",
      "2 2.85e-05, 1.52e-04, 4.30e-02, 3.14e-01\n",
      "3 2.12e-05, 4.09e-05, 5.50e-02, 9.71e-02\n",
      "4 2.13e-05, 3.31e-05, 6.10e-02, 4.84e-02\n",
      "5 1.34e-04, 2.55e-03, 5.90e-02, 6.15e-02\n",
      "6 2.18e-05, 5.44e-05, 4.80e-02, 2.00e-01\n",
      "7 2.59e-05, 9.41e-05, 1.22e-01, 6.67e-07\n",
      "8 2.52e-05, 8.17e-05, 1.11e-01, 8.74e-06\n",
      "9 8.58e-05, 1.88e-03, 9.40e-02, 2.88e-04\n",
      "advfinetune_0\n",
      "0 2.29e-05, 5.14e-05, 3.70e-02, 5.01e-01\n",
      "1 3.27e-05, 2.29e-04, 7.10e-02, 1.29e-02\n",
      "2 2.93e-05, 1.14e-04, 8.30e-02, 2.03e-03\n",
      "3 4.46e-05, 4.14e-04, 1.36e-01, 1.77e-08\n",
      "4 3.68e-05, 1.82e-04, 9.90e-02, 1.10e-04\n",
      "5 2.61e-05, 9.72e-05, 1.20e-01, 1.08e-06\n",
      "6 2.41e-04, 6.39e-03, 1.28e-01, 1.48e-07\n",
      "7 1.38e-04, 2.34e-03, 1.73e-01, 1.75e-13\n",
      "8 9.24e-05, 1.34e-03, 1.25e-01, 3.17e-07\n",
      "9 7.42e-05, 6.77e-04, 1.09e-01, 1.36e-05\n",
      "finetune_1\n",
      "0 2.62e-05, 7.07e-05, 4.30e-02, 3.14e-01\n",
      "1 4.93e-05, 9.10e-04, 4.90e-02, 1.81e-01\n",
      "2 5.40e-05, 5.45e-04, 9.50e-02, 2.39e-04\n",
      "3 3.04e-05, 2.36e-04, 5.20e-02, 1.34e-01\n",
      "4 2.66e-05, 9.99e-05, 6.60e-02, 2.56e-02\n",
      "5 2.26e-05, 4.68e-05, 4.50e-02, 2.63e-01\n",
      "6 2.45e-05, 1.07e-04, 4.90e-02, 1.81e-01\n",
      "7 2.47e-05, 7.11e-05, 7.40e-02, 8.35e-03\n",
      "8 1.04e-04, 2.08e-03, 1.04e-01, 3.96e-05\n",
      "9 2.66e-05, 1.94e-04, 6.50e-02, 2.92e-02\n",
      "advfinetune_1\n",
      "0 3.37e-05, 1.77e-04, 9.50e-02, 2.39e-04\n",
      "1 3.36e-05, 3.51e-04, 4.00e-02, 4.01e-01\n",
      "2 2.11e-04, 2.71e-03, 1.31e-01, 6.77e-08\n",
      "3 3.79e-05, 2.62e-04, 9.90e-02, 1.10e-04\n",
      "4 4.69e-05, 7.90e-04, 7.90e-02, 3.88e-03\n",
      "5 4.19e-05, 2.81e-04, 7.60e-02, 6.19e-03\n",
      "6 3.59e-05, 2.34e-04, 1.57e-01, 3.61e-11\n",
      "7 9.24e-05, 1.48e-03, 8.80e-02, 8.61e-04\n",
      "8 1.56e-04, 2.35e-03, 1.18e-01, 1.75e-06\n",
      "9 1.06e-04, 1.53e-03, 1.03e-01, 4.87e-05\n",
      "finetune_2\n",
      "0 3.20e-05, 2.84e-04, 4.80e-02, 2.00e-01\n",
      "1 1.31e-04, 3.08e-03, 4.50e-02, 2.63e-01\n",
      "2 2.33e-05, 5.41e-05, 5.00e-02, 1.64e-01\n",
      "3 2.45e-05, 7.77e-05, 9.00e-02, 6.03e-04\n",
      "4 2.27e-05, 5.34e-05, 8.10e-02, 2.82e-03\n",
      "5 2.64e-05, 1.10e-04, 7.60e-02, 6.19e-03\n",
      "6 8.76e-05, 1.88e-03, 7.30e-02, 9.68e-03\n",
      "7 3.18e-05, 2.30e-04, 8.60e-02, 1.22e-03\n",
      "8 2.39e-05, 4.62e-05, 9.40e-02, 2.88e-04\n",
      "9 2.21e-05, 2.73e-05, 1.02e-01, 5.98e-05\n",
      "advfinetune_2\n",
      "0 3.27e-05, 2.19e-04, 6.60e-02, 2.56e-02\n",
      "1 9.91e-05, 2.32e-03, 1.45e-01, 1.39e-09\n",
      "2 9.97e-05, 2.17e-03, 5.90e-02, 6.15e-02\n",
      "3 1.22e-04, 2.34e-03, 2.80e-02, 8.28e-01\n",
      "4 3.45e-05, 2.03e-04, 1.36e-01, 1.77e-08\n",
      "5 2.40e-05, 7.54e-05, 7.60e-02, 6.19e-03\n",
      "6 1.32e-04, 3.04e-03, 1.37e-01, 1.34e-08\n",
      "7 3.84e-05, 3.12e-04, 1.74e-01, 1.24e-13\n",
      "8 8.34e-05, 1.66e-03, 8.50e-02, 1.45e-03\n",
      "9 6.41e-05, 8.40e-04, 1.84e-01, 3.32e-15\n",
      "finetune_3\n",
      "0 3.51e-05, 3.42e-04, 6.70e-02, 2.24e-02\n",
      "1 3.14e-05, 2.59e-04, 8.10e-02, 2.82e-03\n",
      "2 2.36e-05, 6.45e-05, 4.80e-02, 2.00e-01\n",
      "3 1.40e-04, 3.82e-03, 5.30e-02, 1.21e-01\n",
      "4 4.60e-05, 7.29e-04, 6.20e-02, 4.28e-02\n",
      "5 2.75e-05, 1.80e-04, 5.70e-02, 7.76e-02\n",
      "6 2.51e-05, 6.05e-05, 6.20e-02, 4.28e-02\n",
      "7 2.59e-04, 7.50e-03, 5.80e-02, 6.92e-02\n",
      "8 2.24e-05, 3.53e-05, 7.40e-02, 8.35e-03\n",
      "9 2.67e-05, 1.36e-04, 7.30e-02, 9.68e-03\n",
      "advfinetune_3\n",
      "0 2.50e-05, 1.88e-04, 4.90e-02, 1.81e-01\n",
      "1 2.92e-05, 2.41e-04, 6.10e-02, 4.84e-02\n",
      "2 3.46e-05, 3.34e-04, 6.70e-02, 2.24e-02\n",
      "3 2.19e-05, 2.27e-05, 1.32e-01, 5.19e-08\n",
      "4 4.19e-05, 5.34e-04, 1.08e-01, 1.69e-05\n",
      "5 3.18e-05, 1.80e-04, 1.59e-01, 1.91e-11\n",
      "6 4.79e-05, 6.03e-04, 8.30e-02, 2.03e-03\n",
      "7 2.42e-05, 7.57e-05, 1.29e-01, 1.14e-07\n",
      "8 1.37e-04, 3.52e-03, 1.94e-01, 7.25e-17\n",
      "9 6.46e-05, 6.12e-04, 2.08e-01, 2.42e-19\n",
      "finetune_4\n",
      "0 4.41e-05, 2.86e-04, 4.40e-02, 2.88e-01\n",
      "1 3.67e-05, 3.65e-04, 3.90e-02, 4.33e-01\n",
      "2 4.00e-05, 5.54e-04, 8.30e-02, 2.03e-03\n",
      "3 3.89e-05, 4.41e-04, 4.20e-02, 3.41e-01\n",
      "4 2.44e-05, 8.45e-05, 6.80e-02, 1.96e-02\n",
      "5 6.39e-05, 9.15e-04, 4.00e-02, 4.01e-01\n",
      "6 2.95e-05, 2.34e-04, 5.60e-02, 8.69e-02\n",
      "7 5.46e-05, 7.93e-04, 4.80e-02, 2.00e-01\n",
      "8 8.53e-05, 1.92e-03, 1.11e-01, 8.74e-06\n",
      "9 3.15e-05, 2.43e-04, 6.90e-02, 1.71e-02\n",
      "advfinetune_4\n",
      "0 2.24e-05, 4.90e-05, 5.40e-02, 1.08e-01\n",
      "1 6.87e-05, 1.24e-03, 5.20e-02, 1.34e-01\n",
      "2 4.49e-05, 4.34e-04, 9.90e-02, 1.10e-04\n",
      "3 2.35e-05, 6.51e-05, 4.60e-02, 2.41e-01\n",
      "4 5.13e-05, 6.31e-04, 1.19e-01, 1.38e-06\n",
      "5 2.90e-05, 8.37e-05, 1.02e-01, 5.98e-05\n",
      "6 4.22e-05, 3.09e-04, 6.60e-02, 2.56e-02\n",
      "7 5.92e-05, 4.27e-04, 1.58e-01, 2.63e-11\n",
      "8 1.12e-04, 2.01e-03, 1.24e-01, 4.07e-07\n",
      "9 5.56e-05, 5.53e-04, 1.50e-01, 3.14e-10\n",
      "finetune_5\n",
      "0 2.41e-05, 6.40e-05, 8.90e-02, 7.21e-04\n",
      "1 2.84e-05, 1.48e-04, 7.00e-02, 1.49e-02\n",
      "2 2.40e-05, 6.49e-05, 4.40e-02, 2.88e-01\n",
      "3 2.28e-05, 6.39e-05, 6.40e-02, 3.33e-02\n",
      "4 2.57e-05, 8.64e-05, 7.10e-02, 1.29e-02\n",
      "5 3.93e-05, 2.55e-04, 4.30e-02, 3.14e-01\n",
      "6 2.47e-05, 1.01e-04, 6.40e-02, 3.33e-02\n",
      "7 2.95e-05, 2.78e-04, 6.70e-02, 2.24e-02\n",
      "8 1.99e-05, 2.71e-05, 6.00e-02, 5.46e-02\n",
      "9 3.45e-05, 2.27e-04, 1.04e-01, 3.96e-05\n",
      "10 2.23e-05, 6.30e-05, 7.20e-02, 1.12e-02\n",
      "11 1.66e-04, 2.97e-03, 8.30e-02, 2.03e-03\n",
      "12 2.17e-05, 5.18e-05, 7.50e-02, 7.20e-03\n",
      "13 2.49e-05, 5.42e-05, 1.30e-01, 8.80e-08\n",
      "14 3.08e-05, 1.59e-04, 1.01e-01, 7.34e-05\n",
      "15 1.25e-04, 3.02e-03, 1.95e-01, 4.89e-17\n",
      "16 2.55e-05, 5.37e-05, 1.32e-01, 5.19e-08\n",
      "17 7.30e-05, 1.23e-03, 1.21e-01, 8.51e-07\n",
      "18 2.61e-05, 1.28e-04, 1.28e-01, 1.48e-07\n",
      "19 2.44e-05, 3.24e-05, 1.48e-01, 5.73e-10\n",
      "advfinetune_5\n",
      "0 2.71e-05, 9.04e-05, 8.10e-02, 2.82e-03\n",
      "1 2.64e-05, 1.13e-04, 6.00e-02, 5.46e-02\n",
      "2 6.56e-05, 9.40e-04, 5.90e-02, 6.15e-02\n",
      "3 2.90e-05, 1.13e-04, 8.30e-02, 2.03e-03\n",
      "4 2.65e-05, 1.09e-04, 6.60e-02, 2.56e-02\n",
      "5 3.46e-05, 1.28e-04, 8.20e-02, 2.39e-03\n",
      "6 2.62e-05, 1.46e-04, 7.20e-02, 1.12e-02\n",
      "7 4.28e-05, 3.75e-04, 1.69e-01, 6.98e-13\n",
      "8 5.84e-05, 6.51e-04, 1.20e-01, 1.08e-06\n",
      "9 3.18e-05, 2.20e-04, 9.40e-02, 2.88e-04\n",
      "10 1.26e-04, 2.49e-03, 1.18e-01, 1.75e-06\n",
      "11 7.68e-05, 1.51e-03, 1.93e-01, 1.07e-16\n",
      "12 3.63e-05, 2.91e-04, 1.57e-01, 3.61e-11\n",
      "13 4.50e-05, 2.44e-04, 1.72e-01, 2.48e-13\n",
      "14 3.10e-05, 7.45e-05, 2.19e-01, 2.05e-21\n",
      "15 6.71e-05, 5.65e-04, 2.31e-01, 8.47e-24\n",
      "16 1.02e-04, 1.54e-03, 2.19e-01, 2.05e-21\n",
      "17 7.53e-05, 1.18e-03, 1.78e-01, 2.98e-14\n",
      "18 6.38e-05, 6.42e-04, 2.38e-01, 2.99e-25\n",
      "19 1.28e-04, 1.72e-03, 1.65e-01, 2.69e-12\n",
      "finetune_6\n",
      "0 3.65e-05, 4.02e-04, 5.40e-02, 1.08e-01\n",
      "1 2.05e-05, 3.34e-05, 5.50e-02, 9.71e-02\n",
      "2 3.47e-05, 3.75e-04, 4.20e-02, 3.41e-01\n",
      "3 1.94e-05, 2.04e-05, 6.20e-02, 4.28e-02\n",
      "4 3.82e-05, 5.36e-04, 5.30e-02, 1.21e-01\n",
      "5 2.33e-05, 7.14e-05, 5.60e-02, 8.69e-02\n",
      "6 2.66e-05, 1.26e-04, 1.22e-01, 6.67e-07\n",
      "7 5.73e-05, 9.67e-04, 1.00e-01, 8.97e-05\n",
      "8 3.80e-05, 2.85e-04, 1.22e-01, 6.67e-07\n",
      "9 4.69e-05, 6.35e-04, 4.90e-02, 1.81e-01\n",
      "10 3.52e-05, 2.11e-04, 1.04e-01, 3.96e-05\n",
      "11 2.36e-05, 5.73e-05, 1.26e-01, 2.46e-07\n",
      "12 2.73e-05, 1.20e-04, 1.15e-01, 3.53e-06\n",
      "13 5.59e-05, 7.72e-04, 1.15e-01, 3.53e-06\n",
      "14 2.66e-05, 6.31e-05, 1.22e-01, 6.67e-07\n",
      "15 2.96e-05, 1.04e-04, 1.21e-01, 8.51e-07\n",
      "16 2.68e-05, 8.85e-05, 1.35e-01, 2.32e-08\n",
      "17 3.57e-05, 1.92e-04, 1.06e-01, 2.60e-05\n",
      "18 2.23e-05, 4.26e-05, 1.09e-01, 1.36e-05\n",
      "19 2.95e-05, 1.32e-04, 1.32e-01, 5.19e-08\n",
      "advfinetune_6\n",
      "0 5.38e-05, 5.40e-04, 7.20e-02, 1.12e-02\n",
      "1 2.99e-05, 1.82e-04, 1.40e-01, 5.82e-09\n",
      "2 2.70e-05, 8.38e-05, 9.20e-02, 4.19e-04\n",
      "3 2.02e-04, 4.67e-03, 1.03e-01, 4.87e-05\n",
      "4 1.76e-04, 4.13e-03, 1.06e-01, 2.60e-05\n",
      "5 8.81e-05, 1.33e-03, 6.00e-02, 5.46e-02\n",
      "6 3.44e-05, 1.97e-04, 6.30e-02, 3.78e-02\n",
      "7 2.48e-05, 4.12e-05, 1.69e-01, 6.98e-13\n",
      "8 5.86e-05, 5.86e-04, 8.50e-02, 1.45e-03\n",
      "9 2.10e-05, 2.22e-05, 1.16e-01, 2.80e-06\n",
      "10 1.80e-04, 3.38e-03, 1.82e-01, 6.95e-15\n",
      "11 3.62e-05, 1.96e-04, 1.71e-01, 3.51e-13\n",
      "12 1.04e-04, 1.98e-03, 1.68e-01, 9.80e-13\n",
      "13 5.36e-05, 4.47e-04, 1.34e-01, 3.04e-08\n",
      "14 7.32e-05, 6.01e-04, 2.87e-01, 1.09e-36\n",
      "15 1.28e-04, 1.15e-03, 1.95e-01, 4.89e-17\n",
      "16 5.62e-05, 4.39e-04, 1.87e-01, 1.08e-15\n",
      "17 1.78e-04, 1.96e-03, 2.40e-01, 1.13e-25\n",
      "18 2.75e-04, 4.28e-03, 2.86e-01, 1.97e-36\n",
      "19 2.81e-04, 4.79e-03, 2.18e-01, 3.20e-21\n",
      "finetune_7\n",
      "0 5.22e-05, 9.56e-04, 4.50e-02, 2.63e-01\n",
      "1 6.00e-05, 7.90e-04, 5.90e-02, 6.15e-02\n",
      "2 5.40e-05, 8.18e-04, 5.40e-02, 1.08e-01\n",
      "3 2.67e-05, 1.48e-04, 1.19e-01, 1.38e-06\n",
      "4 2.71e-05, 1.10e-04, 8.00e-02, 3.31e-03\n",
      "5 5.64e-05, 7.41e-04, 9.60e-02, 1.97e-04\n",
      "6 5.79e-05, 8.96e-04, 6.80e-02, 1.96e-02\n",
      "7 2.71e-05, 8.56e-05, 1.01e-01, 7.34e-05\n",
      "8 2.43e-05, 1.25e-04, 6.70e-02, 2.24e-02\n",
      "9 1.48e-04, 3.00e-03, 9.60e-02, 1.97e-04\n",
      "10 2.51e-05, 7.49e-05, 9.70e-02, 1.62e-04\n",
      "11 2.32e-05, 2.79e-05, 1.41e-01, 4.39e-09\n",
      "12 3.33e-05, 2.33e-04, 1.02e-01, 5.98e-05\n",
      "13 9.51e-05, 2.13e-03, 1.23e-01, 5.21e-07\n",
      "14 2.65e-05, 6.16e-05, 1.21e-01, 8.51e-07\n",
      "15 3.30e-05, 2.34e-04, 1.25e-01, 3.17e-07\n",
      "16 3.37e-05, 1.37e-04, 1.63e-01, 5.21e-12\n",
      "17 6.00e-05, 1.11e-03, 1.20e-01, 1.08e-06\n",
      "18 2.30e-05, 3.68e-05, 1.56e-01, 4.94e-11\n",
      "19 2.72e-05, 8.98e-05, 1.51e-01, 2.32e-10\n",
      "advfinetune_7\n",
      "0 9.59e-05, 1.47e-03, 6.20e-02, 4.28e-02\n",
      "1 4.94e-05, 6.53e-04, 8.70e-02, 1.03e-03\n",
      "2 2.52e-05, 6.44e-05, 4.90e-02, 1.81e-01\n",
      "3 2.67e-05, 9.03e-05, 4.70e-02, 2.19e-01\n",
      "4 2.26e-05, 4.11e-05, 5.70e-02, 7.76e-02\n",
      "5 2.25e-04, 5.99e-03, 1.22e-01, 6.67e-07\n",
      "6 6.40e-05, 8.44e-04, 9.80e-02, 1.33e-04\n",
      "7 2.48e-05, 1.16e-04, 7.10e-02, 1.29e-02\n",
      "8 6.80e-05, 9.11e-04, 1.15e-01, 3.53e-06\n",
      "9 3.29e-05, 1.21e-04, 1.52e-01, 1.71e-10\n",
      "10 3.65e-05, 2.39e-04, 1.45e-01, 1.39e-09\n",
      "11 2.38e-05, 6.60e-05, 1.03e-01, 4.87e-05\n",
      "12 4.92e-05, 4.22e-04, 1.45e-01, 1.39e-09\n",
      "13 2.71e-04, 5.31e-03, 1.56e-01, 4.94e-11\n",
      "14 9.00e-05, 1.06e-03, 1.95e-01, 4.89e-17\n",
      "15 4.70e-05, 5.71e-04, 1.78e-01, 2.98e-14\n",
      "16 4.25e-05, 2.57e-04, 2.04e-01, 1.29e-18\n",
      "17 8.21e-05, 1.03e-03, 1.68e-01, 9.80e-13\n",
      "18 2.21e-04, 2.65e-03, 2.16e-01, 7.73e-21\n",
      "19 7.85e-05, 5.88e-04, 1.89e-01, 5.03e-16\n",
      "finetune_8\n",
      "0 8.09e-05, 1.82e-03, 4.70e-02, 2.19e-01\n",
      "1 5.55e-05, 6.91e-04, 4.00e-02, 4.01e-01\n",
      "2 2.10e-05, 3.99e-05, 4.30e-02, 3.14e-01\n",
      "3 2.24e-05, 6.15e-05, 4.40e-02, 2.88e-01\n",
      "4 8.99e-05, 2.21e-03, 4.20e-02, 3.41e-01\n",
      "5 2.25e-05, 4.73e-05, 6.90e-02, 1.71e-02\n",
      "6 3.82e-05, 2.97e-04, 7.40e-02, 8.35e-03\n",
      "7 2.70e-05, 8.37e-05, 7.70e-02, 5.31e-03\n",
      "8 5.14e-05, 7.78e-04, 6.20e-02, 4.28e-02\n",
      "9 1.97e-05, 2.68e-05, 7.10e-02, 1.29e-02\n",
      "10 2.58e-05, 1.28e-04, 1.25e-01, 3.17e-07\n",
      "11 5.35e-05, 7.54e-04, 8.00e-02, 3.31e-03\n",
      "12 3.00e-05, 1.76e-04, 9.70e-02, 1.62e-04\n",
      "13 6.04e-05, 1.06e-03, 1.10e-01, 1.09e-05\n",
      "14 2.23e-05, 3.61e-05, 1.11e-01, 8.74e-06\n",
      "15 5.48e-05, 8.79e-04, 1.13e-01, 5.58e-06\n",
      "16 6.47e-05, 1.25e-03, 1.24e-01, 4.07e-07\n",
      "17 3.57e-05, 2.04e-04, 1.62e-01, 7.22e-12\n",
      "18 5.65e-05, 7.53e-04, 6.30e-02, 3.78e-02\n",
      "19 3.52e-05, 3.48e-04, 1.43e-01, 2.48e-09\n",
      "advfinetune_8\n",
      "0 1.97e-04, 4.59e-03, 1.25e-01, 3.17e-07\n",
      "1 1.24e-04, 2.36e-03, 3.90e-02, 4.33e-01\n",
      "2 2.18e-05, 3.35e-05, 9.70e-02, 1.62e-04\n",
      "3 4.71e-05, 5.70e-04, 1.28e-01, 1.48e-07\n",
      "4 7.13e-05, 1.37e-03, 4.80e-02, 2.00e-01\n",
      "5 2.74e-05, 9.83e-05, 1.22e-01, 6.67e-07\n",
      "6 8.23e-05, 1.33e-03, 1.23e-01, 5.21e-07\n",
      "7 3.24e-05, 1.24e-04, 1.56e-01, 4.94e-11\n",
      "8 8.04e-05, 9.76e-04, 1.18e-01, 1.75e-06\n",
      "9 5.99e-04, 1.69e-02, 1.33e-01, 3.98e-08\n",
      "10 7.32e-05, 9.08e-04, 1.53e-01, 1.26e-10\n",
      "11 2.06e-04, 5.15e-03, 1.25e-01, 3.17e-07\n",
      "12 4.61e-05, 4.62e-04, 1.67e-01, 1.37e-12\n",
      "13 7.95e-05, 1.42e-03, 1.61e-01, 1.00e-11\n",
      "14 4.51e-04, 1.17e-02, 2.16e-01, 7.73e-21\n",
      "15 3.16e-05, 1.20e-04, 1.40e-01, 5.82e-09\n",
      "16 3.24e-05, 1.77e-04, 1.66e-01, 1.92e-12\n",
      "17 4.10e-05, 2.68e-04, 2.02e-01, 2.93e-18\n",
      "18 7.18e-05, 6.83e-04, 2.43e-01, 2.58e-26\n",
      "19 2.05e-04, 2.23e-03, 1.97e-01, 2.21e-17\n",
      "finetune_9\n",
      "0 1.50e-04, 3.99e-03, 4.20e-02, 3.41e-01\n",
      "1 2.58e-05, 1.28e-04, 4.40e-02, 2.88e-01\n",
      "2 2.14e-05, 4.25e-05, 4.70e-02, 2.19e-01\n",
      "3 3.26e-05, 3.35e-04, 7.50e-02, 7.20e-03\n",
      "4 3.39e-05, 2.85e-04, 7.70e-02, 5.31e-03\n",
      "5 9.23e-05, 2.00e-03, 9.00e-02, 6.03e-04\n",
      "6 3.16e-05, 1.76e-04, 3.40e-02, 6.10e-01\n",
      "7 2.58e-05, 7.33e-05, 9.90e-02, 1.10e-04\n",
      "8 8.36e-05, 1.70e-03, 8.90e-02, 7.21e-04\n",
      "9 3.48e-05, 3.43e-04, 6.50e-02, 2.92e-02\n",
      "10 2.33e-05, 6.94e-05, 6.40e-02, 3.33e-02\n",
      "11 4.63e-05, 5.15e-04, 9.70e-02, 1.62e-04\n",
      "12 3.80e-05, 3.05e-04, 1.30e-01, 8.80e-08\n",
      "13 3.99e-05, 4.04e-04, 1.12e-01, 6.99e-06\n",
      "14 2.60e-05, 7.45e-05, 1.18e-01, 1.75e-06\n",
      "15 4.45e-05, 2.82e-04, 1.21e-01, 8.51e-07\n",
      "16 2.98e-05, 1.28e-04, 1.10e-01, 1.09e-05\n",
      "17 1.27e-04, 2.23e-03, 1.39e-01, 7.71e-09\n",
      "18 2.82e-05, 1.76e-04, 1.65e-01, 2.69e-12\n",
      "19 2.89e-05, 1.03e-04, 1.55e-01, 6.76e-11\n",
      "advfinetune_9\n",
      "0 2.24e-04, 4.38e-03, 1.36e-01, 1.77e-08\n",
      "1 2.90e-05, 2.34e-04, 4.20e-02, 3.41e-01\n",
      "2 5.77e-05, 1.05e-03, 6.30e-02, 3.78e-02\n",
      "3 2.12e-05, 3.32e-05, 5.80e-02, 6.92e-02\n",
      "4 2.75e-05, 1.36e-04, 9.50e-02, 2.39e-04\n",
      "5 3.18e-05, 2.01e-04, 1.22e-01, 6.67e-07\n",
      "6 4.30e-05, 4.09e-04, 1.12e-01, 6.99e-06\n",
      "7 4.19e-05, 3.60e-04, 9.20e-02, 4.19e-04\n",
      "8 4.56e-05, 5.62e-04, 1.18e-01, 1.75e-06\n",
      "9 4.74e-05, 3.22e-04, 1.69e-01, 6.98e-13\n",
      "10 2.69e-05, 7.10e-05, 1.38e-01, 1.02e-08\n",
      "11 2.30e-04, 3.61e-03, 1.86e-01, 1.57e-15\n",
      "12 7.66e-05, 1.13e-03, 1.30e-01, 8.80e-08\n",
      "13 1.40e-04, 2.43e-03, 1.75e-01, 8.69e-14\n",
      "14 4.95e-05, 2.99e-04, 1.74e-01, 1.24e-13\n",
      "15 1.56e-04, 2.31e-03, 2.36e-01, 7.86e-25\n",
      "16 4.83e-05, 4.14e-04, 9.00e-02, 6.03e-04\n",
      "17 4.77e-04, 1.17e-02, 2.19e-01, 2.05e-21\n",
      "18 1.13e-04, 1.30e-03, 1.76e-01, 6.09e-14\n",
      "19 7.97e-05, 6.28e-04, 2.05e-01, 8.49e-19\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for k, vlist in output_dict['finetune'].items():\n",
    "        print(k)\n",
    "        finetune_errors[k] = []\n",
    "        for i, v in enumerate(vlist):\n",
    "            out = autoencoder(v).softmax(dim=1)\n",
    "            errors = torch.sum((v - out)**2, dim=1).numpy()\n",
    "            stats, pv = sps.ks_2samp(errors, vic_errors)\n",
    "            finetune_errors[k].append(errors)\n",
    "            print(i, \"{:.2e}, {:.2e}, {:.2e}, {:.2e}\".format(\n",
    "            np.mean(errors), np.std(errors), stats, pv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "beneficial-stockholm",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.15e-02, 1.54e-01, 8.59e-01, 0.00e+00\n",
      "1 1.87e-01, 2.77e-01, 9.68e-01, 0.00e+00\n",
      "2 2.88e-04, 2.26e-03, 4.76e-01, 7.17e-103\n",
      "3 1.69e-01, 9.89e-02, 9.92e-01, 0.00e+00\n",
      "4 2.11e-01, 1.12e-01, 9.84e-01, 0.00e+00\n",
      "5 3.27e-05, 5.45e-05, 4.55e-01, 1.12e-93\n",
      "6 2.43e-01, 9.96e-02, 9.79e-01, 0.00e+00\n",
      "7 1.90e-01, 1.36e-01, 9.84e-01, 0.00e+00\n",
      "8 1.56e-01, 1.10e-01, 9.62e-01, 0.00e+00\n",
      "9 7.01e-02, 1.59e-01, 8.17e-01, 0.00e+00\n",
      "10 2.84e-01, 1.42e-01, 9.85e-01, 0.00e+00\n",
      "11 2.88e-01, 9.13e-02, 9.84e-01, 0.00e+00\n",
      "12 1.16e-01, 1.69e-01, 9.49e-01, 0.00e+00\n",
      "13 1.41e-02, 4.61e-02, 6.53e-01, 1.33e-201\n",
      "14 9.01e-02, 1.01e-01, 9.61e-01, 0.00e+00\n",
      "15 9.86e-02, 1.12e-01, 9.36e-01, 0.00e+00\n",
      "16 4.89e-03, 3.43e-02, 6.11e-01, 1.91e-174\n",
      "17 7.47e-02, 9.43e-02, 9.60e-01, 0.00e+00\n",
      "18 1.60e-03, 7.97e-03, 5.70e-01, 2.71e-150\n",
      "19 1.51e-01, 1.58e-01, 9.77e-01, 0.00e+00\n",
      "20 1.80e-01, 1.10e-01, 9.82e-01, 0.00e+00\n",
      "21 2.71e-01, 6.06e-02, 1.00e+00, 0.00e+00\n",
      "22 2.92e-01, 1.75e-01, 9.91e-01, 0.00e+00\n",
      "23 3.27e-05, 1.73e-04, 1.81e-01, 1.00e-14\n",
      "24 2.49e-01, 8.40e-02, 9.95e-01, 0.00e+00\n",
      "25 7.74e-03, 6.06e-02, 4.41e-01, 8.13e-88\n",
      "26 1.81e-02, 5.41e-02, 5.89e-01, 3.30e-161\n",
      "27 7.87e-02, 1.31e-01, 9.79e-01, 0.00e+00\n",
      "28 2.12e-02, 5.37e-02, 7.70e-01, 2.43e-292\n",
      "29 4.00e-02, 7.96e-02, 8.70e-01, 0.00e+00\n",
      "30 6.07e-02, 8.99e-02, 9.04e-01, 0.00e+00\n",
      "31 1.94e-01, 7.68e-02, 9.98e-01, 0.00e+00\n",
      "32 7.36e-04, 7.21e-03, 3.18e-01, 4.31e-45\n",
      "33 2.96e-01, 2.47e-01, 9.76e-01, 0.00e+00\n",
      "34 1.33e-03, 1.11e-02, 7.20e-01, 1.40e-250\n",
      "35 1.82e-01, 9.54e-02, 9.89e-01, 0.00e+00\n",
      "36 3.54e-01, 1.52e-02, 1.00e+00, 0.00e+00\n",
      "37 7.48e-04, 9.85e-03, 9.90e-02, 1.10e-04\n",
      "38 3.20e-01, 2.15e-01, 9.53e-01, 0.00e+00\n",
      "39 2.25e-02, 5.40e-02, 9.21e-01, 0.00e+00\n",
      "40 1.86e-01, 1.79e-01, 9.72e-01, 0.00e+00\n",
      "41 6.65e-02, 1.11e-01, 8.55e-01, 0.00e+00\n",
      "42 1.69e-05, 2.34e-05, 1.07e-01, 2.10e-05\n",
      "43 1.05e-01, 1.42e-01, 8.82e-01, 0.00e+00\n",
      "44 2.52e-01, 9.96e-02, 9.77e-01, 0.00e+00\n",
      "45 9.39e-05, 1.52e-03, 1.62e-01, 7.22e-12\n",
      "46 1.47e-03, 9.06e-03, 4.05e-01, 1.04e-73\n",
      "47 4.29e-05, 3.68e-06, 8.15e-01, 0.00e+00\n",
      "48 5.27e-04, 6.93e-03, 1.36e-01, 1.77e-08\n",
      "49 4.98e-03, 1.72e-02, 6.89e-01, 5.32e-227\n",
      "50 3.20e-05, 1.14e-04, 2.97e-01, 2.68e-39\n",
      "51 1.10e-01, 1.19e-01, 9.57e-01, 0.00e+00\n",
      "52 1.04e-01, 1.73e-01, 9.59e-01, 0.00e+00\n",
      "53 2.85e-01, 9.65e-02, 9.78e-01, 0.00e+00\n",
      "54 1.48e-01, 1.28e-01, 8.50e-01, 0.00e+00\n",
      "55 4.27e-01, 2.44e-01, 9.99e-01, 0.00e+00\n",
      "56 4.72e-02, 8.69e-02, 6.46e-01, 6.72e-197\n",
      "57 4.52e-01, 2.84e-01, 9.94e-01, 0.00e+00\n",
      "58 4.68e-02, 6.87e-02, 9.82e-01, 0.00e+00\n",
      "59 1.94e-01, 1.12e-01, 9.66e-01, 0.00e+00\n",
      "60 2.76e-03, 2.38e-02, 3.26e-01, 2.07e-47\n",
      "61 3.33e-01, 1.11e-01, 9.99e-01, 0.00e+00\n",
      "62 1.82e-01, 1.97e-01, 9.40e-01, 0.00e+00\n",
      "63 2.60e-02, 7.90e-02, 8.17e-01, 0.00e+00\n",
      "64 5.22e-02, 7.84e-02, 9.16e-01, 0.00e+00\n",
      "65 2.69e-02, 7.11e-02, 8.77e-01, 0.00e+00\n",
      "66 4.36e-03, 2.23e-02, 7.09e-01, 5.05e-242\n",
      "67 2.74e-01, 2.23e-01, 9.90e-01, 0.00e+00\n",
      "68 2.89e-03, 2.18e-02, 5.48e-01, 3.09e-138\n",
      "69 3.01e-01, 1.92e-01, 9.96e-01, 0.00e+00\n",
      "70 3.17e-01, 1.23e-01, 9.99e-01, 0.00e+00\n",
      "71 1.01e-01, 1.34e-01, 9.20e-01, 0.00e+00\n",
      "72 9.49e-02, 1.11e-01, 8.87e-01, 0.00e+00\n",
      "73 3.55e-02, 8.45e-02, 7.59e-01, 9.97e-283\n",
      "74 3.06e-01, 1.15e-01, 9.99e-01, 0.00e+00\n",
      "75 1.22e-02, 3.58e-02, 5.98e-01, 1.52e-166\n",
      "76 5.97e-03, 3.32e-02, 5.07e-01, 2.37e-117\n",
      "77 3.36e-01, 3.38e-02, 9.99e-01, 0.00e+00\n",
      "78 1.04e-01, 1.13e-01, 8.59e-01, 0.00e+00\n",
      "79 3.93e-03, 2.30e-02, 7.62e-01, 2.52e-285\n",
      "80 2.84e-01, 1.07e-01, 9.82e-01, 0.00e+00\n",
      "81 4.68e-02, 1.09e-01, 8.09e-01, 0.00e+00\n",
      "82 3.26e-03, 2.15e-02, 3.43e-01, 1.52e-52\n",
      "83 1.87e-05, 1.24e-05, 2.31e-01, 8.47e-24\n",
      "84 1.45e-02, 5.49e-02, 7.71e-01, 3.16e-293\n",
      "85 1.01e-02, 3.74e-02, 6.13e-01, 1.11e-175\n",
      "86 2.25e-02, 6.23e-02, 7.98e-01, 5.35e-318\n",
      "87 1.64e-04, 1.98e-03, 5.23e-01, 2.92e-125\n",
      "88 1.31e-01, 1.35e-01, 9.70e-01, 0.00e+00\n",
      "89 9.19e-02, 1.81e-01, 8.64e-01, 0.00e+00\n",
      "90 1.93e-01, 1.30e-01, 9.54e-01, 0.00e+00\n",
      "91 2.78e-01, 2.04e-01, 9.94e-01, 0.00e+00\n",
      "92 1.08e-02, 3.79e-02, 4.38e-01, 1.37e-86\n",
      "93 1.00e-01, 1.07e-01, 9.09e-01, 0.00e+00\n",
      "94 4.51e-01, 1.72e-01, 9.93e-01, 0.00e+00\n",
      "95 9.52e-03, 3.90e-02, 6.16e-01, 1.51e-177\n",
      "96 1.72e-01, 1.30e-01, 9.06e-01, 0.00e+00\n",
      "97 3.83e-02, 8.20e-02, 7.84e-01, 5.99e-305\n",
      "98 2.54e-02, 7.08e-02, 7.87e-01, 1.05e-307\n",
      "99 1.28e-01, 9.60e-02, 9.74e-01, 0.00e+00\n",
      "100 2.53e-01, 1.68e-01, 9.76e-01, 0.00e+00\n",
      "101 4.02e-01, 2.46e-01, 9.91e-01, 0.00e+00\n",
      "102 7.25e-02, 1.39e-01, 9.11e-01, 0.00e+00\n",
      "103 2.61e-01, 1.63e-01, 9.82e-01, 0.00e+00\n",
      "104 5.39e-03, 3.26e-02, 4.11e-01, 5.76e-76\n",
      "105 2.92e-01, 7.08e-02, 9.93e-01, 0.00e+00\n",
      "106 2.23e-02, 5.53e-02, 8.21e-01, 0.00e+00\n",
      "107 1.87e-01, 1.03e-01, 9.77e-01, 0.00e+00\n",
      "108 2.74e-01, 8.08e-02, 9.94e-01, 0.00e+00\n",
      "109 5.28e-02, 1.12e-01, 8.03e-01, 8.89e-323\n",
      "110 5.05e-01, 1.09e-01, 9.98e-01, 0.00e+00\n",
      "111 1.35e-01, 1.27e-01, 9.81e-01, 0.00e+00\n",
      "112 5.14e-01, 2.77e-01, 9.96e-01, 0.00e+00\n",
      "113 2.78e-01, 1.62e-01, 9.91e-01, 0.00e+00\n",
      "114 1.72e-01, 1.01e-01, 9.82e-01, 0.00e+00\n",
      "115 8.32e-04, 8.33e-03, 2.91e-01, 1.01e-37\n",
      "116 2.01e-02, 5.31e-02, 6.86e-01, 8.34e-225\n",
      "117 1.79e-01, 1.74e-01, 9.84e-01, 0.00e+00\n",
      "118 3.04e-01, 5.61e-02, 1.00e+00, 0.00e+00\n",
      "119 1.02e-01, 1.05e-01, 8.78e-01, 0.00e+00\n",
      "120 3.53e-01, 1.26e-02, 1.00e+00, 0.00e+00\n",
      "121 9.08e-02, 1.32e-01, 9.63e-01, 0.00e+00\n",
      "122 4.51e-01, 1.95e-01, 1.00e+00, 0.00e+00\n",
      "123 1.33e-01, 2.24e-01, 9.07e-01, 0.00e+00\n",
      "124 3.32e-01, 2.21e-01, 9.95e-01, 0.00e+00\n",
      "125 1.57e-01, 1.01e-01, 9.47e-01, 0.00e+00\n",
      "126 1.26e-01, 1.12e-01, 8.97e-01, 0.00e+00\n",
      "127 9.01e-03, 3.58e-02, 6.01e-01, 2.40e-168\n",
      "128 2.87e-01, 2.78e-01, 9.88e-01, 0.00e+00\n",
      "129 3.02e-03, 2.39e-02, 2.89e-01, 3.35e-37\n",
      "130 4.30e-01, 2.50e-01, 1.00e+00, 0.00e+00\n",
      "131 6.18e-05, 7.86e-04, 5.35e-01, 2.15e-131\n",
      "132 3.02e-01, 2.64e-01, 9.76e-01, 0.00e+00\n",
      "133 6.38e-03, 3.18e-02, 5.06e-01, 7.24e-117\n",
      "134 5.84e-02, 9.52e-02, 7.85e-01, 7.25e-306\n",
      "135 2.55e-01, 1.03e-01, 9.85e-01, 0.00e+00\n",
      "136 4.03e-02, 9.43e-02, 8.22e-01, 0.00e+00\n",
      "137 6.45e-02, 9.11e-02, 7.67e-01, 1.08e-289\n",
      "138 1.25e-01, 1.53e-01, 9.50e-01, 0.00e+00\n",
      "139 1.74e-02, 4.63e-02, 8.06e-01, 0.00e+00\n",
      "140 1.48e-02, 4.53e-02, 8.35e-01, 0.00e+00\n",
      "141 3.48e-02, 7.52e-02, 9.07e-01, 0.00e+00\n",
      "142 3.58e-01, 7.61e-03, 1.00e+00, 0.00e+00\n",
      "143 2.79e-01, 6.83e-02, 9.94e-01, 0.00e+00\n",
      "144 7.84e-02, 1.38e-01, 8.99e-01, 0.00e+00\n",
      "145 1.89e-02, 5.16e-02, 6.97e-01, 6.28e-233\n",
      "146 8.61e-03, 4.24e-02, 4.92e-01, 3.31e-110\n",
      "147 3.40e-01, 4.90e-02, 1.00e+00, 0.00e+00\n",
      "148 4.12e-01, 3.20e-01, 9.93e-01, 0.00e+00\n",
      "149 3.62e-04, 2.87e-03, 5.17e-01, 2.93e-122\n",
      "150 1.16e-02, 4.80e-02, 5.38e-01, 5.91e-133\n",
      "151 2.18e-03, 1.20e-02, 5.55e-01, 5.25e-142\n",
      "152 3.74e-02, 8.39e-02, 8.57e-01, 0.00e+00\n",
      "153 2.93e-01, 7.95e-02, 9.95e-01, 0.00e+00\n",
      "154 9.24e-02, 1.30e-01, 9.64e-01, 0.00e+00\n",
      "155 2.48e-01, 8.80e-02, 9.86e-01, 0.00e+00\n",
      "156 1.97e-01, 1.15e-01, 9.78e-01, 0.00e+00\n",
      "157 5.39e-01, 3.09e-01, 9.98e-01, 0.00e+00\n",
      "158 8.24e-03, 4.11e-02, 5.73e-01, 5.50e-152\n",
      "159 1.74e-01, 9.92e-02, 9.44e-01, 0.00e+00\n",
      "160 7.41e-03, 2.73e-02, 7.60e-01, 1.36e-283\n",
      "161 1.57e-01, 1.15e-01, 9.50e-01, 0.00e+00\n",
      "162 1.58e-01, 1.19e-01, 9.71e-01, 0.00e+00\n",
      "163 1.05e+00, 3.40e-01, 9.99e-01, 0.00e+00\n",
      "164 9.94e-04, 1.28e-02, 4.28e-01, 1.45e-82\n",
      "165 3.36e-01, 4.17e-02, 9.98e-01, 0.00e+00\n",
      "166 2.21e-02, 5.21e-02, 6.61e-01, 4.51e-207\n",
      "167 2.38e-01, 9.12e-02, 9.82e-01, 0.00e+00\n",
      "168 9.46e-02, 1.94e-01, 8.76e-01, 0.00e+00\n",
      "169 1.82e-02, 5.01e-02, 6.59e-01, 1.07e-205\n",
      "170 3.27e-02, 1.13e-01, 6.22e-01, 2.59e-181\n",
      "171 5.02e-02, 8.44e-02, 8.19e-01, 0.00e+00\n",
      "172 3.42e-01, 4.35e-02, 9.96e-01, 0.00e+00\n",
      "173 2.15e-01, 1.09e-01, 9.83e-01, 0.00e+00\n",
      "174 7.90e-02, 1.07e-01, 8.75e-01, 0.00e+00\n",
      "175 3.02e-01, 8.72e-02, 9.90e-01, 0.00e+00\n",
      "176 7.00e-02, 1.14e-01, 8.74e-01, 0.00e+00\n",
      "177 2.56e-01, 1.06e-01, 9.78e-01, 0.00e+00\n",
      "178 1.53e-01, 1.68e-01, 9.08e-01, 0.00e+00\n",
      "179 3.23e-02, 8.44e-02, 7.66e-01, 8.14e-289\n",
      "180 1.65e-01, 1.40e-01, 9.84e-01, 0.00e+00\n",
      "181 2.76e-01, 8.79e-02, 9.87e-01, 0.00e+00\n",
      "182 1.20e-01, 1.46e-01, 9.22e-01, 0.00e+00\n",
      "183 1.03e-02, 4.29e-02, 8.17e-01, 0.00e+00\n",
      "184 1.98e-01, 1.15e-01, 9.70e-01, 0.00e+00\n",
      "185 1.63e-04, 1.59e-03, 9.60e-02, 1.97e-04\n",
      "186 2.14e-01, 1.78e-01, 9.87e-01, 0.00e+00\n",
      "187 2.90e-02, 4.90e-02, 9.56e-01, 0.00e+00\n",
      "188 2.94e-01, 1.71e-01, 9.95e-01, 0.00e+00\n",
      "189 2.95e-01, 8.91e-02, 9.90e-01, 0.00e+00\n",
      "190 2.26e-01, 1.01e-01, 9.86e-01, 0.00e+00\n",
      "191 8.40e-01, 2.94e-01, 1.00e+00, 0.00e+00\n",
      "192 5.81e-01, 2.98e-01, 9.94e-01, 0.00e+00\n",
      "193 1.89e-01, 1.17e-01, 9.83e-01, 0.00e+00\n",
      "194 1.08e-01, 9.46e-02, 9.41e-01, 0.00e+00\n",
      "195 3.54e-01, 1.91e-02, 1.00e+00, 0.00e+00\n",
      "196 2.40e-01, 1.63e-01, 9.94e-01, 0.00e+00\n",
      "197 3.29e-01, 7.49e-02, 1.00e+00, 0.00e+00\n",
      "198 1.33e-03, 9.82e-03, 4.46e-01, 6.94e-90\n",
      "199 1.23e-01, 1.63e-01, 9.83e-01, 0.00e+00\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i, v in enumerate(output_dict['indep']):\n",
    "        out = autoencoder(v).softmax(dim=1)\n",
    "        errors = torch.sum((v - out)**2, dim=1).numpy()\n",
    "        indep_errors.append(errors)\n",
    "        stats, pv = sps.ks_2samp(errors, vic_errors)\n",
    "        print(i, \"{:.2e}, {:.2e}, {:.2e}, {:.2e}\".format(\n",
    "            np.mean(errors), np.std(errors), stats, pv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "described-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(autoencoder.state_dict(), '../results/hash/cifar10/autoencoder.pt')\n",
    "pickle.dump((output_dict, finetune_errors, indep_errors), open(\"../results/hash/cifar10/output_dict.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-auckland",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-insulin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "respective-distribution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00025885057\n"
     ]
    }
   ],
   "source": [
    "finetune_mean_errors = []\n",
    "for k, vlist in finetune_errors.items():\n",
    "    if int(k.split('_')[-1])<5:\n",
    "        for v in vlist:\n",
    "            finetune_mean_errors.append(np.mean(v))\n",
    "print(np.max(finetune_mean_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "forty-prescription",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_finetune_mean_errors = []\n",
    "for k, vlist in finetune_errors.items():\n",
    "    if int(k.split('_')[-1])>4:\n",
    "        for v in vlist:\n",
    "            test_finetune_mean_errors.append(np.mean(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "pending-rwanda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194\n"
     ]
    }
   ],
   "source": [
    "# True positive\n",
    "test_finetune_mean_errors = np.array(test_finetune_mean_errors)\n",
    "print(len(test_finetune_mean_errors[test_finetune_mean_errors<=np.max(finetune_mean_errors)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "alike-cooking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "test_finetune_mean_errors = np.array(test_finetune_mean_errors)\n",
    "print(len(test_finetune_mean_errors[test_finetune_mean_errors>np.max(finetune_mean_errors)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "valuable-shell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True N\n",
      " 190\n",
      "False P\n",
      " 10\n"
     ]
    }
   ],
   "source": [
    "test_indep_mean_errors = []\n",
    "for v in indep_errors:\n",
    "    test_indep_mean_errors.append(np.mean(v))\n",
    "test_indep_mean_errors = np.array(test_indep_mean_errors)\n",
    "print(\"True N\\n\", len(test_indep_mean_errors[test_indep_mean_errors>np.max(finetune_mean_errors)]))\n",
    "print(\"False P\\n\", len(test_indep_mean_errors[test_indep_mean_errors<np.max(finetune_mean_errors)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "super-requirement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(190 + 194) / 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-rendering",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-companion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "modern-paste",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import compute_accuracy, get_test_dataloader_cifar10\n",
    "# test_loader = get_test_dataloader_cifar10((0, 0, 0), (1, 1, 1))\n",
    "# acc, _, _ = compute_accuracy(original_net, test_loader, 'cpu')\n",
    "# accquant, _, _ = compute_accuracy(quant_net, test_loader, 'cpu')\n",
    "# acc20, _, _ = compute_accuracy(pruned_model_2, test_loader, 'cpu')\n",
    "# acc40, _, _ = compute_accuracy(pruned_model_4, test_loader, 'cpu')\n",
    "# acc60, _, _ = compute_accuracy(pruned_model_6, test_loader, 'cpu')\n",
    "# acc80, _, _ = compute_accuracy(pruned_model_8, test_loader, 'cpu')\n",
    "# print(acc, accquant, acc20, acc40, acc60, acc80)\n",
    "\n",
    "# 0.9147 0.9155 0.9151 0.914 0.9106 0.776"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5f3f0cb6f7dc525fd91e30599dc917c9059637fcae9cfadc503d008ae5db0235"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
