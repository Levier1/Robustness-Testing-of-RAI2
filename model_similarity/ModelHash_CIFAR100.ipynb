{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "matched-museum",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.stats as sps\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import prune\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from utils import get_network\n",
    "from conf import settings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "import quant_utils\n",
    "import copy\n",
    "import random\n",
    "seed = 0\n",
    "device = 'cpu'\n",
    "gpu = 'cuda:1'\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hydraulic-vault",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset1_mean, subset1_std = settings.CIFAR100_SUBTRAIN_MEAN[0], settings.CIFAR100_SUBTRAIN_STD[0]\n",
    "\n",
    "def load_model(path, norm=False, dev='cpu'):\n",
    "    vic = get_network('vgg16', False, num_classes=100).to(dev)\n",
    "    vic.load_state_dict(torch.load(path, map_location=dev))\n",
    "    vic.eval()\n",
    "    if norm:\n",
    "        return nn.Sequential(transforms.Normalize(subset1_mean, subset1_std), vic)\n",
    "    return vic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "every-norway",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_prune(net, p=0.2):\n",
    "    copy_net = copy.deepcopy(net)\n",
    "    parameters_to_prune = []\n",
    "    for name, module in copy_net.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            parameters_to_prune.append((module, 'weight'))\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            parameters_to_prune.append((module, 'weight'))\n",
    "    prune.global_unstructured(\n",
    "        tuple(parameters_to_prune),\n",
    "        pruning_method=prune.L1Unstructured,\n",
    "        amount=p,\n",
    "    )\n",
    "    return copy_net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-configuration",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "indian-vampire",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/data1/checkpoint/'\n",
    "related_folder_path = os.path.join(root_path, 'hash/cifar100/')\n",
    "unrelated_folder_path = os.path.join(root_path, 'hash/cifar100/independent')\n",
    "\n",
    "original_path = os.path.join(related_folder_path, 'vgg16_0.pth')\n",
    "quant_path = os.path.join(related_folder_path, 'vgg16_0_quant.pth')\n",
    "\n",
    "finetune_path_dict = {}\n",
    "for fid in range(5):\n",
    "    folder_name = 'finetune_{}'.format(fid)\n",
    "    finetune_path_dict[folder_name] = [\n",
    "        os.path.join(related_folder_path, '{}/finetune_{}.pth'.format(folder_name, i)) for i in range(1, 11)]\n",
    "    folder_name = 'advfinetune_{}'.format(fid)\n",
    "    finetune_path_dict[folder_name] = [\n",
    "        os.path.join(related_folder_path, '{}/finetune_{}.pth'.format(folder_name, i)) for i in range(1, 11)]\n",
    "\n",
    "for fid in range(5, 10):\n",
    "    folder_name = 'finetune_{}'.format(fid)\n",
    "    finetune_path_dict[folder_name] = [\n",
    "        os.path.join(related_folder_path, '{}/finetune_{}.pth'.format(folder_name, i)) for i in range(1, 21)]\n",
    "    folder_name = 'advfinetune_{}'.format(fid)\n",
    "    finetune_path_dict[folder_name] = [\n",
    "        os.path.join(related_folder_path, '{}/finetune_{}.pth'.format(folder_name, i)) for i in range(1, 21)]\n",
    "\n",
    "unrelated_path_list = [os.path.join(unrelated_folder_path, 'model_{}.pth'.format(i)) for i in range(200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "usual-venture",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = True\n",
    "n = 1000\n",
    "input_shape = (n, ) + (3, 32, 32)\n",
    "# checkpoint_out = {}\n",
    "output_dict = {}\n",
    "randf = torch.rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "apparent-monitor",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dt/anaconda3/envs/cvnlp/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448255797/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "100%|██████████| 20/20 [01:15<00:00,  3.78s/it]\n",
      "200it [01:04,  3.08it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # original net\n",
    "    original_net = load_model(original_path, norm=norm, dev=gpu)\n",
    "    output_dict['train'] = original_net(randf(input_shape, device=gpu)).softmax(dim=1).to('cpu')\n",
    "\n",
    "    # quant net\n",
    "    net = quant_utils.load_torchscript_model(quant_path, 'cpu')\n",
    "    if norm:\n",
    "        net = nn.Sequential(transforms.Normalize(subset1_mean, subset1_std), net)\n",
    "    output_dict['quant'] = net(randf(input_shape)).softmax(dim=1)\n",
    "    \n",
    "    # pruning\n",
    "    output_dict['prune'] = {}\n",
    "    for prune_p in [0.8, 0.6, 0.4, 0.2]:\n",
    "        net = global_prune(original_net, prune_p)\n",
    "        output_dict['prune'][prune_p] = net(randf(input_shape, device=gpu)).softmax(dim=1).to('cpu')\n",
    "    \n",
    "    # finetuned model\n",
    "    output_dict['finetune'] = {}\n",
    "    for name, pathlist in tqdm(finetune_path_dict.items()):\n",
    "        output_dict['finetune'][name] = []\n",
    "        for path in pathlist:\n",
    "            net = load_model(path, norm=norm, dev=gpu)\n",
    "            output_dict['finetune'][name].append(net(randf(input_shape, device=gpu)).softmax(dim=1).to('cpu'))\n",
    "\n",
    "    # finetuned model\n",
    "    output_dict['indep'] = []\n",
    "    for i, path in tqdm(enumerate(unrelated_path_list)):\n",
    "        net = load_model(path, norm=norm, dev=gpu)\n",
    "        output_dict['indep'].append(net(randf(input_shape, device=gpu)).softmax(dim=1).to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-colonial",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "christian-demonstration",
   "metadata": {
    "id": "Seq98Ux78Yne"
   },
   "source": [
    "# Train autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "detected-ideal",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 10/10 [00:03<00:00,  2.63it/s, loss=6.2692083347e-10]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "autoencoder = nn.Sequential(\n",
    "                nn.Linear(100, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 100),\n",
    "            ).to(gpu)\n",
    "ae_dataset = torch.utils.data.TensorDataset(output_dict['train'], torch.zeros(len(output_dict['train'])).long()) \n",
    "ae_train_dataloader = torch.utils.data.DataLoader(ae_dataset, \n",
    "                                                  batch_size=128, \n",
    "                                                  shuffle=True, num_workers=8)\n",
    "n_epoch = 10\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=3e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "with tqdm(total=n_epoch, desc=\"train\") as pbar:\n",
    "    for epoch in range(n_epoch):\n",
    "        epoch_loss = 0\n",
    "        for x, y in ae_train_dataloader:\n",
    "            x = x.to(gpu)\n",
    "            outx = autoencoder(x).softmax(dim=1)\n",
    "            loss = criterion(x, outx)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * len(y)\n",
    "#         print(epoch, epoch_loss / len(ae_dataset))\n",
    "        pbar.set_postfix({'loss' : '{0:1.10e}'.format(epoch_loss / len(ae_dataset))}) \n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "injured-alexander",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=100, out_features=64, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=64, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-string",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "incorporate-latex",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 6.14e-08, 5.97e-08\n"
     ]
    }
   ],
   "source": [
    "vic_errors = []\n",
    "with torch.no_grad():\n",
    "    for i in range(1):\n",
    "        randout = original_net(randf(input_shape, device=gpu)).softmax(dim=1).cpu()\n",
    "        out = autoencoder(randout).softmax(dim=1)\n",
    "        vic_errors = torch.sum((randout - out)**2, dim=1).numpy()\n",
    "        print('{}, {:.2e}, {:.2e}'.format(i, np.mean(vic_errors), np.std(vic_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "defensive-ceiling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KstestResult(statistic=0.036, pvalue=0.5362766985932181)\n",
      "6.27e-08, 5.73e-08\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = autoencoder(output_dict['train']).softmax(dim=1)\n",
    "    errors = torch.sum((output_dict['train'] - out)**2, dim=1).numpy()\n",
    "    print(sps.ks_2samp(errors, vic_errors))\n",
    "    print(\"{:.2e}, {:.2e}\".format(np.mean(errors), np.std(errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "floral-compiler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KstestResult(statistic=0.021, pvalue=0.9802627322900355)\n",
      "6.20e-08, 5.99e-08\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    out = autoencoder(output_dict['quant']).softmax(dim=1)\n",
    "    errors = torch.sum((output_dict['quant'] - out)**2, dim=1).numpy()\n",
    "    print(sps.ks_2samp(errors, vic_errors))\n",
    "    print(\"{:.2e}, {:.2e}\".format(np.mean(errors), np.std(errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adjacent-turning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "2.91e-02, 7.80e-03, 1.00e+00, 0.00e+00\n",
      "0.6\n",
      "1.81e-07, 1.31e-07, 6.50e-01, 1.41e-199\n",
      "0.4\n",
      "6.02e-08, 5.25e-08, 3.00e-02, 7.59e-01\n",
      "0.2\n",
      "6.39e-08, 6.96e-08, 5.30e-02, 1.21e-01\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for k, v in output_dict['prune'].items():\n",
    "        print(k)\n",
    "        out = autoencoder(v).softmax(dim=1)\n",
    "        errors = torch.sum((v - out)**2, dim=1).numpy()\n",
    "        stats, pv = sps.ks_2samp(errors, vic_errors)\n",
    "        print(\"{:.2e}, {:.2e}, {:.2e}, {:.2e}\".format(\n",
    "            np.mean(errors), np.std(errors), stats, pv))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "imperial-director",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_errors = {}\n",
    "indep_errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "arabic-distribution",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetune_0\n",
      "0 2.01e-08, 1.76e-08, 5.80e-01, 5.59e-156\n",
      "1 2.44e-08, 2.00e-08, 4.77e-01, 2.55e-103\n",
      "2 3.57e-08, 2.90e-08, 3.01e-01, 2.28e-40\n",
      "3 2.90e-08, 2.62e-08, 4.12e-01, 2.40e-76\n",
      "4 1.61e-08, 1.32e-08, 6.66e-01, 1.54e-210\n",
      "5 2.21e-08, 2.31e-08, 5.59e-01, 3.45e-144\n",
      "6 1.83e-08, 1.64e-08, 6.25e-01, 3.24e-183\n",
      "7 2.27e-08, 2.29e-08, 5.23e-01, 2.92e-125\n",
      "8 2.09e-08, 2.05e-08, 5.73e-01, 5.50e-152\n",
      "9 2.05e-08, 1.54e-08, 5.51e-01, 7.62e-140\n",
      "advfinetune_0\n",
      "0 3.57e-08, 1.12e-07, 4.00e-01, 7.39e-72\n",
      "1 3.45e-08, 3.21e-08, 3.25e-01, 4.07e-47\n",
      "2 3.29e-08, 5.23e-08, 4.16e-01, 7.11e-78\n",
      "3 5.92e-08, 7.62e-08, 1.25e-01, 3.17e-07\n",
      "4 3.22e-08, 3.10e-08, 3.56e-01, 1.16e-56\n",
      "5 3.72e-08, 5.45e-08, 3.64e-01, 2.79e-59\n",
      "6 6.03e-08, 9.17e-08, 1.74e-01, 1.24e-13\n",
      "7 5.20e-08, 7.96e-08, 2.16e-01, 7.73e-21\n",
      "8 5.60e-08, 6.60e-08, 7.40e-02, 8.35e-03\n",
      "9 6.47e-08, 8.38e-08, 1.20e-01, 1.08e-06\n",
      "finetune_1\n",
      "0 2.19e-08, 2.34e-08, 5.41e-01, 1.59e-134\n",
      "1 3.65e-08, 3.39e-08, 3.11e-01, 4.10e-43\n",
      "2 2.74e-08, 2.21e-08, 4.27e-01, 3.62e-82\n",
      "3 1.91e-08, 1.47e-08, 5.92e-01, 5.65e-163\n",
      "4 2.21e-08, 2.02e-08, 5.51e-01, 7.62e-140\n",
      "5 2.06e-08, 1.84e-08, 5.70e-01, 2.71e-150\n",
      "6 2.62e-08, 2.08e-08, 4.60e-01, 8.03e-96\n",
      "7 1.64e-08, 1.23e-08, 6.48e-01, 3.10e-198\n",
      "8 2.09e-08, 1.59e-08, 5.57e-01, 4.28e-143\n",
      "9 1.92e-08, 1.43e-08, 6.01e-01, 2.40e-168\n",
      "advfinetune_1\n",
      "0 2.57e-08, 3.34e-08, 4.95e-01, 1.29e-111\n",
      "1 8.12e-08, 1.78e-07, 1.01e-01, 7.34e-05\n",
      "2 4.22e-08, 6.59e-08, 2.98e-01, 1.45e-39\n",
      "3 4.64e-08, 4.59e-08, 1.89e-01, 5.03e-16\n",
      "4 5.08e-08, 1.50e-07, 2.03e-01, 1.94e-18\n",
      "5 4.30e-08, 5.11e-08, 2.86e-01, 1.97e-36\n",
      "6 5.46e-08, 5.84e-08, 1.39e-01, 7.71e-09\n",
      "7 8.44e-08, 1.34e-07, 8.30e-02, 2.03e-03\n",
      "8 4.25e-08, 5.01e-08, 2.53e-01, 1.63e-28\n",
      "9 2.74e-08, 1.75e-08, 4.29e-01, 5.82e-83\n",
      "finetune_2\n",
      "0 4.51e-08, 4.30e-08, 2.04e-01, 1.29e-18\n",
      "1 2.90e-08, 2.13e-08, 4.02e-01, 1.35e-72\n",
      "2 3.07e-08, 2.67e-08, 3.83e-01, 9.39e-66\n",
      "3 2.84e-08, 3.23e-08, 4.30e-01, 2.32e-83\n",
      "4 2.32e-08, 1.83e-08, 5.25e-01, 2.85e-126\n",
      "5 2.06e-08, 1.63e-08, 5.74e-01, 1.49e-152\n",
      "6 2.97e-08, 2.39e-08, 3.86e-01, 8.26e-67\n",
      "7 2.42e-08, 1.97e-08, 4.90e-01, 2.83e-109\n",
      "8 2.42e-08, 1.82e-08, 4.83e-01, 4.81e-106\n",
      "9 2.15e-08, 1.80e-08, 5.55e-01, 5.25e-142\n",
      "advfinetune_2\n",
      "0 5.30e-08, 5.49e-08, 1.22e-01, 6.67e-07\n",
      "1 6.11e-08, 2.29e-07, 2.29e-01, 2.16e-23\n",
      "2 4.97e-08, 1.17e-07, 1.77e-01, 4.27e-14\n",
      "3 3.18e-08, 5.46e-08, 3.95e-01, 4.95e-70\n",
      "4 4.04e-08, 4.21e-08, 2.93e-01, 3.05e-38\n",
      "5 3.75e-08, 4.20e-08, 3.24e-01, 7.97e-47\n",
      "6 4.14e-08, 6.56e-08, 3.02e-01, 1.22e-40\n",
      "7 3.04e-08, 3.67e-08, 4.00e-01, 7.39e-72\n",
      "8 5.16e-08, 5.90e-08, 1.83e-01, 4.81e-15\n",
      "9 2.37e-08, 2.03e-08, 5.09e-01, 2.53e-118\n",
      "finetune_3\n",
      "0 2.18e-08, 1.83e-08, 5.39e-01, 1.77e-133\n",
      "1 2.78e-08, 2.71e-08, 4.39e-01, 5.37e-87\n",
      "2 3.62e-08, 3.44e-08, 3.25e-01, 4.07e-47\n",
      "3 3.07e-08, 2.53e-08, 3.94e-01, 1.14e-69\n",
      "4 2.68e-08, 2.60e-08, 4.59e-01, 2.17e-95\n",
      "5 2.68e-08, 2.45e-08, 4.65e-01, 5.40e-98\n",
      "6 1.92e-08, 1.55e-08, 6.03e-01, 1.48e-169\n",
      "7 1.91e-08, 2.19e-08, 6.13e-01, 1.11e-175\n",
      "8 1.47e-08, 1.25e-08, 6.93e-01, 5.96e-230\n",
      "9 1.56e-08, 1.23e-08, 6.77e-01, 2.63e-218\n",
      "advfinetune_3\n",
      "0 2.74e-08, 5.50e-08, 4.79e-01, 3.18e-104\n",
      "1 6.73e-08, 1.02e-07, 1.55e-01, 6.76e-11\n",
      "2 5.58e-08, 8.58e-08, 1.87e-01, 1.08e-15\n",
      "3 4.72e-08, 5.55e-08, 2.04e-01, 1.29e-18\n",
      "4 3.74e-08, 5.54e-08, 3.48e-01, 4.15e-54\n",
      "5 6.77e-08, 2.15e-07, 1.13e-01, 5.58e-06\n",
      "6 3.79e-08, 3.96e-08, 2.98e-01, 1.45e-39\n",
      "7 3.28e-08, 4.68e-08, 4.27e-01, 3.62e-82\n",
      "8 3.05e-08, 3.12e-08, 3.94e-01, 1.14e-69\n",
      "9 4.32e-08, 5.53e-08, 2.54e-01, 9.71e-29\n",
      "finetune_4\n",
      "0 2.40e-08, 2.95e-08, 5.16e-01, 9.19e-122\n",
      "1 3.04e-08, 3.66e-08, 4.31e-01, 9.26e-84\n",
      "2 1.71e-08, 1.26e-08, 6.40e-01, 6.35e-193\n",
      "3 3.26e-08, 3.64e-08, 3.82e-01, 2.10e-65\n",
      "4 2.33e-08, 1.99e-08, 5.18e-01, 9.32e-123\n",
      "5 2.75e-08, 2.08e-08, 4.23e-01, 1.36e-80\n",
      "6 3.73e-08, 3.59e-08, 3.19e-01, 2.23e-45\n",
      "7 1.49e-08, 1.32e-08, 6.78e-01, 5.06e-219\n",
      "8 2.29e-08, 1.68e-08, 4.96e-01, 4.36e-112\n",
      "9 2.48e-08, 1.98e-08, 4.85e-01, 5.82e-107\n",
      "advfinetune_4\n",
      "0 1.01e-07, 2.27e-07, 1.47e-01, 7.71e-10\n",
      "1 5.47e-08, 6.86e-08, 1.82e-01, 6.95e-15\n",
      "2 5.97e-08, 1.03e-07, 1.49e-01, 4.25e-10\n",
      "3 5.20e-08, 4.83e-08, 1.49e-01, 4.25e-10\n",
      "4 3.51e-08, 4.43e-08, 3.44e-01, 7.43e-53\n",
      "5 3.68e-08, 3.13e-08, 2.99e-01, 7.85e-40\n",
      "6 3.64e-08, 4.24e-08, 3.48e-01, 4.15e-54\n",
      "7 5.09e-08, 5.76e-08, 1.61e-01, 1.00e-11\n",
      "8 3.79e-08, 3.87e-08, 3.17e-01, 8.32e-45\n",
      "9 7.44e-08, 1.02e-07, 6.30e-02, 3.78e-02\n",
      "finetune_5\n",
      "0 2.91e-08, 2.65e-08, 4.12e-01, 2.40e-76\n",
      "1 2.50e-08, 2.00e-08, 4.68e-01, 2.61e-99\n",
      "2 3.34e-08, 3.32e-08, 3.51e-01, 4.65e-55\n",
      "3 2.17e-08, 1.77e-08, 5.34e-01, 7.08e-131\n",
      "4 4.21e-08, 4.55e-08, 2.36e-01, 7.86e-25\n",
      "5 2.39e-08, 1.99e-08, 5.05e-01, 2.20e-116\n",
      "6 1.84e-08, 1.59e-08, 6.16e-01, 1.51e-177\n",
      "7 1.69e-08, 1.75e-08, 6.42e-01, 3.04e-194\n",
      "8 2.03e-08, 1.86e-08, 5.78e-01, 7.84e-155\n",
      "9 1.64e-08, 1.32e-08, 6.54e-01, 2.78e-202\n",
      "10 1.58e-08, 1.13e-08, 6.80e-01, 1.85e-220\n",
      "11 1.99e-08, 1.57e-08, 5.82e-01, 3.93e-157\n",
      "12 2.35e-08, 2.06e-08, 5.15e-01, 2.87e-121\n",
      "13 2.27e-08, 2.14e-08, 5.25e-01, 2.85e-126\n",
      "14 2.16e-08, 1.62e-08, 5.44e-01, 4.15e-136\n",
      "15 2.11e-08, 1.79e-08, 5.79e-01, 2.10e-155\n",
      "16 2.20e-08, 2.42e-08, 5.57e-01, 4.28e-143\n",
      "17 1.80e-08, 1.69e-08, 6.50e-01, 1.41e-199\n",
      "18 1.63e-08, 1.16e-08, 6.70e-01, 2.42e-213\n",
      "19 1.79e-08, 1.19e-08, 6.36e-01, 2.65e-190\n",
      "advfinetune_5\n",
      "0 3.66e-08, 4.73e-08, 3.37e-01, 1.06e-50\n",
      "1 2.99e-08, 3.71e-08, 4.39e-01, 5.37e-87\n",
      "2 3.51e-08, 3.49e-08, 3.39e-01, 2.60e-51\n",
      "3 4.57e-08, 6.89e-08, 2.83e-01, 1.14e-35\n",
      "4 4.99e-08, 5.39e-08, 1.91e-01, 2.33e-16\n",
      "5 4.62e-08, 4.45e-08, 1.90e-01, 3.43e-16\n",
      "6 4.87e-08, 5.79e-08, 2.14e-01, 1.85e-20\n",
      "7 6.57e-08, 8.88e-08, 7.00e-02, 1.49e-02\n",
      "8 7.54e-08, 2.53e-07, 1.68e-01, 9.80e-13\n",
      "9 4.51e-08, 3.91e-08, 1.92e-01, 1.58e-16\n",
      "10 5.40e-08, 1.16e-07, 2.70e-01, 1.82e-32\n",
      "11 1.01e-07, 1.96e-07, 1.30e-01, 8.80e-08\n",
      "12 9.30e-08, 1.21e-07, 1.30e-01, 8.80e-08\n",
      "13 1.32e-07, 4.09e-07, 1.65e-01, 2.69e-12\n",
      "14 5.08e-08, 5.35e-08, 1.51e-01, 2.32e-10\n",
      "15 4.49e-08, 7.56e-08, 2.87e-01, 1.09e-36\n",
      "16 7.05e-08, 1.95e-07, 2.09e-01, 1.58e-19\n",
      "17 1.82e-07, 4.44e-07, 2.95e-01, 9.08e-39\n",
      "18 8.84e-08, 1.09e-07, 1.34e-01, 3.04e-08\n",
      "19 8.98e-08, 1.25e-07, 1.75e-01, 8.69e-14\n",
      "finetune_6\n",
      "0 2.85e-08, 2.86e-08, 4.18e-01, 1.20e-78\n",
      "1 2.02e-08, 2.37e-08, 5.96e-01, 2.39e-165\n",
      "2 3.98e-08, 3.90e-08, 2.76e-01, 6.34e-34\n",
      "3 2.17e-08, 2.07e-08, 5.47e-01, 1.06e-137\n",
      "4 2.22e-08, 2.11e-08, 5.38e-01, 5.91e-133\n",
      "5 1.92e-08, 3.62e-08, 6.10e-01, 7.89e-174\n",
      "6 1.57e-08, 1.16e-08, 6.72e-01, 9.41e-215\n",
      "7 2.53e-08, 1.94e-08, 4.69e-01, 9.44e-100\n",
      "8 2.32e-08, 2.04e-08, 5.19e-01, 2.96e-123\n",
      "9 1.85e-08, 1.76e-08, 6.25e-01, 3.24e-183\n",
      "10 1.50e-08, 1.29e-08, 7.01e-01, 6.22e-236\n",
      "11 1.85e-08, 1.81e-08, 6.30e-01, 2.04e-186\n",
      "12 1.77e-08, 1.50e-08, 6.40e-01, 6.35e-193\n",
      "13 1.91e-08, 2.24e-08, 6.09e-01, 3.25e-173\n",
      "14 2.19e-08, 1.85e-08, 5.54e-01, 1.83e-141\n",
      "15 1.47e-08, 1.21e-08, 6.98e-01, 1.12e-233\n",
      "16 2.40e-08, 3.06e-08, 5.34e-01, 7.08e-131\n",
      "17 1.33e-08, 9.18e-09, 7.37e-01, 3.09e-264\n",
      "18 1.70e-08, 1.56e-08, 6.62e-01, 9.20e-208\n",
      "19 2.12e-08, 1.73e-08, 5.78e-01, 7.84e-155\n",
      "advfinetune_6\n",
      "0 5.51e-08, 5.52e-08, 1.23e-01, 5.21e-07\n",
      "1 9.35e-08, 2.46e-07, 8.80e-02, 8.61e-04\n",
      "2 7.14e-08, 1.19e-07, 1.38e-01, 1.02e-08\n",
      "3 4.97e-08, 4.99e-08, 1.66e-01, 1.92e-12\n",
      "4 3.69e-08, 4.33e-08, 2.95e-01, 9.08e-39\n",
      "5 4.44e-08, 4.67e-08, 2.37e-01, 4.85e-25\n",
      "6 6.49e-08, 1.32e-07, 1.54e-01, 9.23e-11\n",
      "7 6.69e-08, 9.96e-08, 1.07e-01, 2.10e-05\n",
      "8 6.55e-08, 7.84e-08, 4.90e-02, 1.81e-01\n",
      "9 8.02e-08, 1.67e-07, 7.70e-02, 5.31e-03\n",
      "10 3.68e-08, 4.20e-08, 3.34e-01, 8.61e-50\n",
      "11 5.19e-08, 6.84e-08, 1.41e-01, 4.39e-09\n",
      "12 1.91e-07, 3.80e-07, 3.10e-01, 7.79e-43\n",
      "13 5.45e-08, 6.52e-08, 1.25e-01, 3.17e-07\n",
      "14 1.17e-07, 7.08e-07, 1.31e-01, 6.77e-08\n",
      "15 5.19e-08, 1.43e-07, 2.67e-01, 9.50e-32\n",
      "16 6.15e-08, 7.66e-08, 3.40e-02, 6.10e-01\n",
      "17 6.76e-08, 1.14e-07, 1.04e-01, 3.96e-05\n",
      "18 6.55e-08, 1.16e-07, 1.60e-01, 1.38e-11\n",
      "19 7.59e-08, 7.36e-08, 1.42e-01, 3.30e-09\n",
      "finetune_7\n",
      "0 3.24e-08, 3.49e-08, 3.78e-01, 5.15e-64\n",
      "1 2.62e-08, 5.67e-08, 5.14e-01, 8.95e-121\n",
      "2 2.10e-08, 1.97e-08, 5.56e-01, 1.50e-142\n",
      "3 2.30e-08, 2.07e-08, 5.16e-01, 9.19e-122\n",
      "4 3.39e-08, 2.99e-08, 3.41e-01, 6.32e-52\n",
      "5 3.72e-08, 4.90e-08, 3.15e-01, 3.08e-44\n",
      "6 2.41e-08, 2.63e-08, 5.32e-01, 7.62e-130\n",
      "7 3.08e-08, 3.84e-08, 3.98e-01, 4.00e-71\n",
      "8 2.33e-08, 1.76e-08, 5.11e-01, 2.67e-119\n",
      "9 2.12e-08, 1.98e-08, 5.60e-01, 9.75e-145\n",
      "10 2.07e-08, 2.00e-08, 5.92e-01, 5.65e-163\n",
      "11 2.11e-08, 2.40e-08, 5.84e-01, 2.73e-158\n",
      "12 3.14e-08, 2.56e-08, 3.70e-01, 2.76e-61\n",
      "13 1.89e-08, 1.38e-08, 6.17e-01, 3.60e-178\n",
      "14 1.77e-08, 1.27e-08, 6.33e-01, 2.36e-188\n",
      "15 1.88e-08, 1.64e-08, 5.98e-01, 1.52e-166\n",
      "16 1.85e-08, 1.67e-08, 6.15e-01, 6.36e-177\n",
      "17 2.24e-08, 1.68e-08, 5.41e-01, 1.59e-134\n",
      "18 2.66e-08, 2.22e-08, 4.64e-01, 1.48e-97\n",
      "19 2.16e-08, 1.73e-08, 5.59e-01, 3.45e-144\n",
      "advfinetune_7\n",
      "0 5.77e-08, 6.63e-08, 1.11e-01, 8.74e-06\n",
      "1 4.24e-08, 4.38e-08, 2.23e-01, 3.41e-22\n",
      "2 6.15e-08, 1.78e-07, 1.46e-01, 1.04e-09\n",
      "3 6.49e-08, 1.29e-07, 1.60e-01, 1.38e-11\n",
      "4 6.17e-08, 6.72e-08, 8.60e-02, 1.22e-03\n",
      "5 4.58e-08, 6.46e-08, 2.51e-01, 4.56e-28\n",
      "6 5.27e-08, 5.77e-08, 1.72e-01, 2.48e-13\n",
      "7 7.42e-08, 1.04e-07, 7.30e-02, 9.68e-03\n",
      "8 3.96e-08, 4.03e-08, 2.82e-01, 2.04e-35\n",
      "9 5.35e-08, 8.80e-08, 2.26e-01, 8.66e-23\n",
      "10 4.48e-08, 6.97e-08, 2.97e-01, 2.68e-39\n",
      "11 4.77e-08, 6.24e-08, 2.05e-01, 8.49e-19\n",
      "12 5.46e-08, 5.95e-08, 1.32e-01, 5.19e-08\n",
      "13 6.66e-08, 8.15e-08, 7.50e-02, 7.20e-03\n",
      "14 5.00e-08, 5.56e-08, 1.70e-01, 4.96e-13\n",
      "15 6.55e-08, 8.82e-08, 3.70e-02, 5.01e-01\n",
      "16 7.74e-08, 1.55e-07, 5.80e-02, 6.92e-02\n",
      "17 7.05e-08, 8.50e-08, 5.00e-02, 1.64e-01\n",
      "18 9.13e-08, 1.01e-07, 1.81e-01, 1.00e-14\n",
      "19 1.35e-07, 1.82e-07, 2.93e-01, 3.05e-38\n",
      "finetune_8\n",
      "0 3.25e-08, 4.58e-08, 3.74e-01, 1.21e-62\n",
      "1 2.90e-08, 2.46e-08, 4.13e-01, 1.00e-76\n",
      "2 2.43e-08, 1.93e-08, 4.91e-01, 9.69e-110\n",
      "3 2.02e-08, 1.66e-08, 5.61e-01, 2.75e-145\n",
      "4 2.37e-08, 1.72e-08, 4.90e-01, 2.83e-109\n",
      "5 2.80e-08, 3.34e-08, 4.55e-01, 1.12e-93\n",
      "6 1.44e-08, 1.47e-08, 6.92e-01, 3.28e-229\n",
      "7 2.58e-08, 2.47e-08, 4.69e-01, 9.44e-100\n",
      "8 2.29e-08, 2.16e-08, 5.25e-01, 2.85e-126\n",
      "9 2.18e-08, 2.03e-08, 5.57e-01, 4.28e-143\n",
      "10 2.39e-08, 3.30e-08, 5.35e-01, 2.15e-131\n",
      "11 2.94e-08, 2.88e-08, 4.09e-01, 3.29e-75\n",
      "12 1.50e-08, 1.23e-08, 6.88e-01, 2.88e-226\n",
      "13 2.49e-08, 2.05e-08, 4.95e-01, 1.29e-111\n",
      "14 1.77e-08, 1.45e-08, 6.47e-01, 1.45e-197\n",
      "15 1.74e-08, 1.56e-08, 6.40e-01, 6.35e-193\n",
      "16 2.16e-08, 1.95e-08, 5.60e-01, 9.75e-145\n",
      "17 2.14e-08, 1.66e-08, 5.33e-01, 2.33e-130\n",
      "18 1.58e-08, 1.23e-08, 6.71e-01, 4.78e-214\n",
      "19 2.34e-08, 2.09e-08, 5.20e-01, 9.36e-124\n",
      "advfinetune_8\n",
      "0 4.31e-08, 1.09e-07, 3.07e-01, 5.27e-42\n",
      "1 5.67e-08, 1.43e-07, 2.09e-01, 1.58e-19\n",
      "2 3.26e-08, 3.96e-08, 3.69e-01, 6.00e-61\n",
      "3 6.15e-08, 7.89e-08, 1.17e-01, 2.21e-06\n",
      "4 3.11e-08, 4.22e-08, 4.07e-01, 1.86e-74\n",
      "5 3.02e-08, 2.86e-08, 3.98e-01, 4.00e-71\n",
      "6 1.02e-07, 1.29e-07, 1.99e-01, 9.91e-18\n",
      "7 2.03e-08, 2.94e-08, 6.00e-01, 9.59e-168\n",
      "8 3.67e-08, 8.00e-08, 3.40e-01, 1.28e-51\n",
      "9 6.37e-08, 1.24e-07, 1.12e-01, 6.99e-06\n",
      "10 3.10e-08, 2.50e-08, 3.84e-01, 4.18e-66\n",
      "11 4.50e-08, 4.65e-08, 2.32e-01, 5.29e-24\n",
      "12 4.28e-08, 4.17e-08, 2.67e-01, 9.50e-32\n",
      "13 5.55e-08, 6.05e-08, 1.18e-01, 1.75e-06\n",
      "14 4.02e-08, 4.71e-08, 3.02e-01, 1.22e-40\n",
      "15 3.78e-08, 3.46e-08, 2.95e-01, 9.08e-39\n",
      "16 4.80e-08, 5.95e-08, 2.19e-01, 2.05e-21\n",
      "17 7.47e-08, 1.26e-07, 7.90e-02, 3.88e-03\n",
      "18 6.21e-08, 6.95e-08, 3.90e-02, 4.33e-01\n",
      "19 4.55e-08, 5.30e-08, 2.35e-01, 1.27e-24\n",
      "finetune_9\n",
      "0 2.76e-08, 2.41e-08, 4.28e-01, 1.45e-82\n",
      "1 3.03e-08, 2.49e-08, 3.90e-01, 3.13e-68\n",
      "2 4.84e-08, 4.96e-08, 1.92e-01, 1.58e-16\n",
      "3 3.41e-08, 3.85e-08, 3.63e-01, 5.98e-59\n",
      "4 1.87e-08, 1.69e-08, 6.09e-01, 3.25e-173\n",
      "5 2.77e-08, 2.45e-08, 4.37e-01, 3.51e-86\n",
      "6 2.58e-08, 2.43e-08, 4.71e-01, 1.23e-100\n",
      "7 2.16e-08, 2.73e-08, 5.80e-01, 5.59e-156\n",
      "8 4.06e-08, 4.68e-08, 2.82e-01, 2.04e-35\n",
      "9 2.14e-08, 1.65e-08, 5.42e-01, 4.72e-135\n",
      "10 3.03e-08, 2.58e-08, 4.12e-01, 2.40e-76\n",
      "11 1.82e-08, 1.26e-08, 6.21e-01, 1.11e-180\n",
      "12 1.40e-08, 1.29e-08, 7.17e-01, 3.17e-248\n",
      "13 2.64e-08, 2.43e-08, 4.67e-01, 7.18e-99\n",
      "14 2.46e-08, 1.98e-08, 4.87e-01, 6.97e-108\n",
      "15 1.70e-08, 1.19e-08, 6.42e-01, 3.04e-194\n",
      "16 1.69e-08, 1.43e-08, 6.56e-01, 1.21e-203\n",
      "17 1.55e-08, 1.08e-08, 6.83e-01, 1.26e-222\n",
      "18 1.75e-08, 1.30e-08, 6.36e-01, 2.65e-190\n",
      "19 1.90e-08, 1.70e-08, 6.27e-01, 1.72e-184\n",
      "advfinetune_9\n",
      "0 4.40e-08, 4.82e-08, 2.36e-01, 7.86e-25\n",
      "1 4.00e-08, 4.37e-08, 2.99e-01, 7.85e-40\n",
      "2 2.50e-08, 2.94e-08, 5.00e-01, 5.52e-114\n",
      "3 4.54e-08, 5.66e-08, 2.36e-01, 7.86e-25\n",
      "4 3.14e-08, 8.20e-08, 4.87e-01, 6.97e-108\n",
      "5 3.26e-08, 3.37e-08, 3.75e-01, 5.53e-63\n",
      "6 2.76e-08, 3.44e-08, 4.75e-01, 2.02e-102\n",
      "7 3.74e-08, 3.46e-08, 2.87e-01, 1.09e-36\n",
      "8 5.01e-08, 1.03e-07, 2.08e-01, 2.42e-19\n",
      "9 4.08e-08, 3.05e-08, 2.22e-01, 5.36e-22\n",
      "10 4.68e-08, 4.72e-08, 2.03e-01, 1.94e-18\n",
      "11 9.93e-08, 1.87e-07, 1.17e-01, 2.21e-06\n",
      "12 3.97e-08, 4.91e-08, 3.09e-01, 1.48e-42\n",
      "13 5.10e-08, 5.07e-08, 1.41e-01, 4.39e-09\n",
      "14 6.57e-08, 9.45e-08, 8.60e-02, 1.22e-03\n",
      "15 6.02e-08, 5.47e-08, 4.70e-02, 2.19e-01\n",
      "16 5.21e-08, 5.58e-08, 1.44e-01, 1.86e-09\n",
      "17 1.18e-07, 3.85e-07, 9.90e-02, 1.10e-04\n",
      "18 1.09e-07, 2.54e-07, 1.66e-01, 1.92e-12\n",
      "19 3.72e-08, 3.65e-08, 3.12e-01, 2.15e-43\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for k, vlist in output_dict['finetune'].items():\n",
    "        print(k)\n",
    "        finetune_errors[k] = []\n",
    "        for i, v in enumerate(vlist):\n",
    "            out = autoencoder(v).softmax(dim=1)\n",
    "            errors = torch.sum((v - out)**2, dim=1).numpy()\n",
    "            stats, pv = sps.ks_2samp(errors, vic_errors)\n",
    "            finetune_errors[k].append(errors)\n",
    "            print(i, \"{:.2e}, {:.2e}, {:.2e}, {:.2e}\".format(\n",
    "            np.mean(errors), np.std(errors), stats, pv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "conscious-quilt",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.23e+00, 4.07e-01, 1.00e+00, 0.00e+00\n",
      "1 4.51e-07, 6.31e-06, 1.97e-01, 2.21e-17\n",
      "2 9.81e-03, 9.65e-02, 8.75e-01, 0.00e+00\n",
      "3 5.67e-03, 7.61e-02, 7.72e-01, 4.08e-294\n",
      "4 1.01e-01, 2.00e-01, 1.00e+00, 0.00e+00\n",
      "5 8.24e-05, 1.10e-03, 7.86e-01, 8.73e-307\n",
      "6 1.13e-04, 1.14e-03, 9.26e-01, 0.00e+00\n",
      "7 8.05e-08, 2.17e-07, 1.44e-01, 1.86e-09\n",
      "8 2.43e-01, 3.02e-01, 1.00e+00, 0.00e+00\n",
      "9 2.24e-02, 1.42e-01, 7.72e-01, 4.08e-294\n",
      "10 4.18e-02, 1.61e-01, 9.92e-01, 0.00e+00\n",
      "11 1.92e-01, 3.76e-01, 9.94e-01, 0.00e+00\n",
      "12 4.53e-05, 1.31e-03, 2.45e-01, 9.52e-27\n",
      "13 2.02e-05, 4.21e-04, 5.67e-01, 1.30e-148\n",
      "14 1.75e-01, 3.72e-01, 9.97e-01, 0.00e+00\n",
      "15 3.86e-06, 7.34e-05, 2.60e-01, 4.15e-30\n",
      "16 4.45e-01, 5.62e-01, 9.99e-01, 0.00e+00\n",
      "17 1.89e-02, 1.12e-01, 8.88e-01, 0.00e+00\n",
      "18 1.38e+00, 6.29e-01, 1.00e+00, 0.00e+00\n",
      "19 6.00e-01, 5.89e-01, 9.99e-01, 0.00e+00\n",
      "20 3.83e-01, 5.31e-01, 9.93e-01, 0.00e+00\n",
      "21 1.32e-01, 3.44e-01, 9.90e-01, 0.00e+00\n",
      "22 1.65e-02, 9.26e-02, 9.38e-01, 0.00e+00\n",
      "23 1.15e-07, 1.92e-06, 7.11e-01, 1.46e-243\n",
      "24 1.81e-02, 1.15e-01, 8.23e-01, 0.00e+00\n",
      "25 2.36e-02, 1.29e-01, 9.98e-01, 0.00e+00\n",
      "26 1.06e-04, 2.09e-03, 2.60e-01, 4.15e-30\n",
      "27 5.87e-02, 1.75e-01, 1.00e+00, 0.00e+00\n",
      "28 3.05e-01, 5.02e-01, 9.95e-01, 0.00e+00\n",
      "29 5.39e-02, 2.20e-01, 9.84e-01, 0.00e+00\n",
      "30 7.54e-03, 7.43e-02, 7.62e-01, 2.52e-285\n",
      "31 6.97e-03, 4.02e-02, 1.00e+00, 0.00e+00\n",
      "32 6.22e-01, 6.17e-01, 9.99e-01, 0.00e+00\n",
      "33 2.42e-05, 3.11e-05, 9.99e-01, 0.00e+00\n",
      "34 2.68e-01, 4.94e-01, 8.62e-01, 0.00e+00\n",
      "35 5.47e-07, 3.48e-06, 4.41e-01, 8.13e-88\n",
      "36 1.70e-06, 1.24e-05, 7.68e-01, 1.42e-290\n",
      "37 2.81e-06, 1.86e-05, 6.03e-01, 1.48e-169\n",
      "38 5.56e-08, 2.17e-07, 4.54e-01, 2.99e-93\n",
      "39 1.68e-05, 3.50e-04, 5.88e-01, 1.27e-160\n",
      "40 1.46e-07, 2.83e-07, 3.96e-01, 2.15e-70\n",
      "41 2.89e-01, 5.33e-01, 9.75e-01, 0.00e+00\n",
      "42 1.22e+00, 7.37e-01, 9.98e-01, 0.00e+00\n",
      "43 4.87e-05, 4.47e-04, 7.37e-01, 3.09e-264\n",
      "44 1.72e-06, 4.34e-06, 8.59e-01, 0.00e+00\n",
      "45 5.32e-08, 4.43e-08, 1.16e-01, 2.80e-06\n",
      "46 2.01e-08, 2.39e-08, 6.16e-01, 1.51e-177\n",
      "47 5.28e-03, 3.38e-02, 9.95e-01, 0.00e+00\n",
      "48 1.12e-04, 1.11e-03, 6.96e-01, 3.51e-232\n",
      "49 1.50e-05, 7.61e-05, 7.58e-01, 7.24e-282\n",
      "50 1.59e-03, 3.27e-02, 6.70e-01, 2.42e-213\n",
      "51 4.27e-03, 3.67e-02, 9.65e-01, 0.00e+00\n",
      "52 3.60e-06, 4.37e-05, 8.08e-01, 0.00e+00\n",
      "53 1.60e-03, 2.27e-02, 8.40e-01, 0.00e+00\n",
      "54 1.71e-08, 9.67e-09, 6.55e-01, 5.81e-203\n",
      "55 6.95e-03, 8.37e-02, 4.26e-01, 9.01e-82\n",
      "56 1.05e-01, 3.23e-01, 9.60e-01, 0.00e+00\n",
      "57 1.47e-01, 3.49e-01, 9.95e-01, 0.00e+00\n",
      "58 4.18e-05, 7.99e-04, 2.94e-01, 1.67e-38\n",
      "59 1.53e-02, 9.08e-02, 1.00e+00, 0.00e+00\n",
      "60 7.71e-03, 6.99e-02, 8.37e-01, 0.00e+00\n",
      "61 5.84e-04, 4.04e-03, 9.79e-01, 0.00e+00\n",
      "62 9.23e-01, 6.39e-01, 9.99e-01, 0.00e+00\n",
      "63 2.32e-04, 3.17e-03, 9.94e-01, 0.00e+00\n",
      "64 1.99e-06, 1.28e-05, 5.64e-01, 6.05e-147\n",
      "65 4.55e-02, 1.77e-01, 8.84e-01, 0.00e+00\n",
      "66 6.99e-06, 2.30e-05, 9.76e-01, 0.00e+00\n",
      "67 1.19e-07, 9.02e-08, 4.97e-01, 1.47e-112\n",
      "68 3.38e-06, 8.33e-05, 4.62e-01, 1.09e-96\n",
      "69 9.91e-04, 1.15e-02, 9.44e-01, 0.00e+00\n",
      "70 9.85e-03, 8.37e-02, 9.41e-01, 0.00e+00\n",
      "71 3.27e-02, 1.44e-01, 9.82e-01, 0.00e+00\n",
      "72 9.28e-07, 1.55e-05, 7.26e-01, 2.44e-255\n",
      "73 4.24e-03, 2.98e-02, 9.37e-01, 0.00e+00\n",
      "74 1.32e+00, 4.85e-01, 1.00e+00, 0.00e+00\n",
      "75 1.53e-06, 7.83e-06, 8.11e-01, 0.00e+00\n",
      "76 8.06e-08, 8.24e-07, 3.08e-01, 2.79e-42\n",
      "77 9.01e-01, 5.42e-01, 1.00e+00, 0.00e+00\n",
      "78 1.09e-01, 3.05e-01, 9.84e-01, 0.00e+00\n",
      "79 1.67e-02, 1.25e-01, 8.26e-01, 0.00e+00\n",
      "80 1.07e-01, 2.41e-01, 9.99e-01, 0.00e+00\n",
      "81 6.84e-01, 5.96e-01, 9.96e-01, 0.00e+00\n",
      "82 1.23e-01, 2.59e-01, 1.00e+00, 0.00e+00\n",
      "83 1.52e+00, 5.59e-01, 1.00e+00, 0.00e+00\n",
      "84 3.51e-03, 2.88e-02, 9.95e-01, 0.00e+00\n",
      "85 2.60e-06, 3.45e-05, 4.06e-01, 4.40e-74\n",
      "86 9.25e-02, 2.45e-01, 9.86e-01, 0.00e+00\n",
      "87 1.59e-02, 8.86e-02, 9.78e-01, 0.00e+00\n",
      "88 4.32e-03, 3.61e-02, 8.69e-01, 0.00e+00\n",
      "89 4.57e-01, 4.76e-01, 9.99e-01, 0.00e+00\n",
      "90 9.21e-02, 2.55e-01, 9.81e-01, 0.00e+00\n",
      "91 7.04e-06, 1.48e-05, 9.91e-01, 0.00e+00\n",
      "92 8.48e-04, 9.90e-03, 9.24e-01, 0.00e+00\n",
      "93 1.65e-01, 3.28e-01, 9.88e-01, 0.00e+00\n",
      "94 6.47e-03, 6.58e-02, 8.02e-01, 8.20e-322\n",
      "95 4.47e-06, 1.30e-04, 4.10e-01, 1.38e-75\n",
      "96 3.29e-01, 3.79e-01, 1.00e+00, 0.00e+00\n",
      "97 1.12e-03, 7.46e-03, 9.93e-01, 0.00e+00\n",
      "98 3.54e-07, 1.79e-06, 6.24e-01, 1.40e-182\n",
      "99 4.36e-02, 1.85e-01, 9.43e-01, 0.00e+00\n",
      "100 9.09e-04, 1.53e-02, 8.55e-01, 0.00e+00\n",
      "101 2.94e-03, 2.67e-02, 9.71e-01, 0.00e+00\n",
      "102 3.21e-03, 4.19e-02, 9.47e-01, 0.00e+00\n",
      "103 4.41e-07, 6.49e-06, 5.07e-01, 2.37e-117\n",
      "104 1.92e-01, 3.58e-01, 9.95e-01, 0.00e+00\n",
      "105 2.98e-04, 4.93e-03, 8.64e-01, 0.00e+00\n",
      "106 1.25e-07, 1.71e-07, 2.68e-01, 5.49e-32\n",
      "107 1.75e-01, 4.48e-01, 9.25e-01, 0.00e+00\n",
      "108 2.86e-01, 5.05e-01, 9.97e-01, 0.00e+00\n",
      "109 2.39e-08, 9.65e-08, 7.00e-01, 3.53e-235\n",
      "110 3.34e-03, 5.53e-02, 6.31e-01, 4.64e-187\n",
      "111 4.81e-06, 4.38e-05, 7.69e-01, 1.86e-291\n",
      "112 5.95e-05, 1.14e-03, 4.73e-01, 1.58e-101\n",
      "113 4.08e-06, 9.35e-05, 4.73e-01, 1.58e-101\n",
      "114 2.19e-07, 1.75e-06, 1.36e-01, 1.77e-08\n",
      "115 2.19e-06, 1.30e-05, 4.58e-01, 5.84e-95\n",
      "116 2.23e-04, 5.42e-03, 6.96e-01, 3.51e-232\n",
      "117 8.37e-04, 2.10e-02, 4.33e-01, 1.46e-84\n",
      "118 1.50e-01, 3.22e-01, 1.00e+00, 0.00e+00\n",
      "119 2.61e-02, 1.24e-01, 9.80e-01, 0.00e+00\n",
      "120 8.98e-03, 6.22e-02, 9.54e-01, 0.00e+00\n",
      "121 2.03e-06, 2.73e-05, 3.49e-01, 2.00e-54\n",
      "122 2.00e-01, 4.80e-01, 9.46e-01, 0.00e+00\n",
      "123 8.07e-06, 1.57e-04, 4.34e-01, 5.76e-85\n",
      "124 1.79e-01, 3.30e-01, 9.98e-01, 0.00e+00\n",
      "125 1.81e-02, 1.25e-01, 8.86e-01, 0.00e+00\n",
      "126 6.98e-02, 2.10e-01, 1.00e+00, 0.00e+00\n",
      "127 2.08e-02, 1.23e-01, 8.92e-01, 0.00e+00\n",
      "128 1.07e-07, 1.33e-07, 2.40e-01, 1.13e-25\n",
      "129 7.02e-05, 1.00e-03, 8.51e-01, 0.00e+00\n",
      "130 3.87e-07, 5.29e-07, 6.66e-01, 1.54e-210\n",
      "131 4.00e-05, 1.26e-03, 4.97e-01, 1.47e-112\n",
      "132 1.58e-03, 4.81e-02, 7.20e-01, 1.40e-250\n",
      "133 4.53e-05, 1.15e-03, 4.19e-01, 4.94e-79\n",
      "134 2.89e-04, 4.27e-03, 9.43e-01, 0.00e+00\n",
      "135 1.86e-03, 3.22e-02, 7.59e-01, 9.97e-283\n",
      "136 9.09e-01, 4.81e-01, 1.00e+00, 0.00e+00\n",
      "137 6.95e-05, 9.14e-04, 8.73e-01, 0.00e+00\n",
      "138 4.50e-01, 4.82e-01, 9.97e-01, 0.00e+00\n",
      "139 2.47e-08, 1.28e-07, 7.32e-01, 3.65e-260\n",
      "140 2.55e-06, 4.38e-06, 9.82e-01, 0.00e+00\n",
      "141 3.63e-03, 4.48e-02, 8.68e-01, 0.00e+00\n",
      "142 3.97e-04, 4.26e-03, 9.72e-01, 0.00e+00\n",
      "143 2.88e-08, 2.92e-07, 8.22e-01, 0.00e+00\n",
      "144 1.62e-05, 2.62e-04, 9.33e-01, 0.00e+00\n",
      "145 8.47e-01, 5.65e-01, 1.00e+00, 0.00e+00\n",
      "146 5.27e-05, 5.75e-04, 7.56e-01, 3.77e-280\n",
      "147 5.40e-06, 2.69e-05, 8.93e-01, 0.00e+00\n",
      "148 1.86e-04, 5.77e-03, 7.24e-01, 9.57e-254\n",
      "149 3.57e-03, 4.43e-02, 8.76e-01, 0.00e+00\n",
      "150 2.30e-01, 4.61e-01, 9.98e-01, 0.00e+00\n",
      "151 1.54e-03, 2.82e-02, 8.77e-01, 0.00e+00\n",
      "152 1.15e-05, 7.97e-05, 8.93e-01, 0.00e+00\n",
      "153 2.00e-04, 3.73e-03, 9.47e-01, 0.00e+00\n",
      "154 3.32e-05, 2.11e-04, 9.85e-01, 0.00e+00\n",
      "155 8.21e-08, 1.05e-07, 1.06e-01, 2.60e-05\n",
      "156 2.13e-02, 1.12e-01, 9.94e-01, 0.00e+00\n",
      "157 2.95e-06, 1.17e-05, 9.34e-01, 0.00e+00\n",
      "158 1.32e-02, 9.00e-02, 8.41e-01, 0.00e+00\n",
      "159 2.21e-05, 6.49e-04, 6.71e-01, 4.78e-214\n",
      "160 1.71e-02, 1.28e-01, 8.86e-01, 0.00e+00\n",
      "161 5.28e-02, 1.89e-01, 9.83e-01, 0.00e+00\n",
      "162 3.77e-06, 2.49e-05, 6.37e-01, 5.89e-191\n",
      "163 2.37e-04, 3.70e-04, 1.00e+00, 0.00e+00\n",
      "164 6.02e-07, 1.92e-06, 6.97e-01, 6.28e-233\n",
      "165 9.41e-02, 2.57e-01, 9.78e-01, 0.00e+00\n",
      "166 1.36e-05, 2.31e-04, 7.09e-01, 5.05e-242\n",
      "167 6.49e-04, 9.98e-03, 3.36e-01, 2.14e-50\n",
      "168 1.60e-03, 4.25e-02, 6.50e-01, 1.41e-199\n",
      "169 9.64e-09, 4.63e-09, 8.70e-01, 0.00e+00\n",
      "170 7.27e-01, 6.68e-01, 1.00e+00, 0.00e+00\n",
      "171 7.98e-04, 7.72e-03, 9.83e-01, 0.00e+00\n",
      "172 1.84e-08, 1.31e-08, 6.54e-01, 2.78e-202\n",
      "173 9.13e-05, 1.08e-03, 6.57e-01, 2.51e-204\n",
      "174 5.18e-01, 5.64e-01, 9.96e-01, 0.00e+00\n",
      "175 5.25e-09, 1.18e-08, 9.30e-01, 0.00e+00\n",
      "176 9.82e-02, 2.49e-01, 1.00e+00, 0.00e+00\n",
      "177 5.34e-01, 6.87e-01, 9.98e-01, 0.00e+00\n",
      "178 2.50e-02, 1.32e-01, 9.83e-01, 0.00e+00\n",
      "179 1.17e-01, 3.13e-01, 9.89e-01, 0.00e+00\n",
      "180 2.34e-02, 1.26e-01, 8.97e-01, 0.00e+00\n",
      "181 2.62e-02, 1.68e-01, 9.86e-01, 0.00e+00\n",
      "182 1.22e-01, 2.77e-01, 9.89e-01, 0.00e+00\n",
      "183 9.06e-07, 1.12e-05, 2.68e-01, 5.49e-32\n",
      "184 4.26e-01, 4.75e-01, 1.00e+00, 0.00e+00\n",
      "185 7.56e-06, 9.01e-05, 4.19e-01, 4.94e-79\n",
      "186 1.09e-01, 2.04e-01, 1.00e+00, 0.00e+00\n",
      "187 9.55e-05, 1.41e-03, 6.40e-01, 6.35e-193\n",
      "188 4.05e-06, 7.43e-05, 1.84e-01, 3.32e-15\n",
      "189 1.11e-02, 8.19e-02, 9.22e-01, 0.00e+00\n",
      "190 8.36e-04, 2.39e-02, 2.59e-01, 7.05e-30\n",
      "191 2.39e-03, 5.21e-02, 8.33e-01, 0.00e+00\n",
      "192 2.30e-03, 2.96e-02, 9.88e-01, 0.00e+00\n",
      "193 1.27e-05, 3.24e-04, 5.82e-01, 3.93e-157\n",
      "194 3.38e-01, 4.20e-01, 1.00e+00, 0.00e+00\n",
      "195 2.46e-02, 1.01e-01, 1.00e+00, 0.00e+00\n",
      "196 6.40e-03, 7.11e-02, 9.88e-01, 0.00e+00\n",
      "197 3.93e-05, 7.78e-04, 7.84e-01, 5.99e-305\n",
      "198 3.43e-08, 1.21e-07, 5.56e-01, 1.50e-142\n",
      "199 5.25e-01, 5.90e-01, 1.00e+00, 0.00e+00\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i, v in enumerate(output_dict['indep']):\n",
    "        out = autoencoder(v).softmax(dim=1)\n",
    "        errors = torch.sum((v - out)**2, dim=1).numpy()\n",
    "        indep_errors.append(errors)\n",
    "        stats, pv = sps.ks_2samp(errors, vic_errors)\n",
    "        print(i, \"{:.2e}, {:.2e}, {:.2e}, {:.2e}\".format(\n",
    "            np.mean(errors), np.std(errors), stats, pv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-boston",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-words",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "neural-strengthening",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(autoencoder.state_dict(), '../results/hash/cifar100/autoencoder.pt')\n",
    "pickle.dump((output_dict, finetune_errors, indep_errors), open(\"../results/hash/cifar100/output_dict.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "valued-silicon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0050122e-07\n"
     ]
    }
   ],
   "source": [
    "finetune_mean_errors = []\n",
    "for k, vlist in finetune_errors.items():\n",
    "    if int(k.split('_')[-1])<5:\n",
    "        for v in vlist:\n",
    "            finetune_mean_errors.append(np.mean(v))\n",
    "print(np.max(finetune_mean_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "recreational-recall",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_finetune_mean_errors = []\n",
    "for k, vlist in finetune_errors.items():\n",
    "    if int(k.split('_')[-1])>4:\n",
    "        for v in vlist:\n",
    "            \n",
    "            test_finetune_mean_errors.append(np.mean(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "registered-lawsuit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191\n"
     ]
    }
   ],
   "source": [
    "# True positive\n",
    "test_finetune_mean_errors = np.array(test_finetune_mean_errors)\n",
    "print(len(test_finetune_mean_errors[test_finetune_mean_errors<=np.max(finetune_mean_errors)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "extensive-proposal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "test_finetune_mean_errors = np.array(test_finetune_mean_errors)\n",
    "print(len(test_finetune_mean_errors[test_finetune_mean_errors>np.max(finetune_mean_errors)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "classified-tonight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True N\n",
      " 186\n",
      "False P\n",
      " 14\n"
     ]
    }
   ],
   "source": [
    "test_indep_mean_errors = []\n",
    "for v in indep_errors:\n",
    "    test_indep_mean_errors.append(np.mean(v))\n",
    "test_indep_mean_errors = np.array(test_indep_mean_errors)\n",
    "print(\"True N\\n\", len(test_indep_mean_errors[test_indep_mean_errors>np.max(finetune_mean_errors)]))\n",
    "print(\"False P\\n\", len(test_indep_mean_errors[test_indep_mean_errors<np.max(finetune_mean_errors)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-trade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-belfast",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-enterprise",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-western",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-today",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-saying",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "single-reply",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import compute_accuracy, get_test_dataloader_cifar100\n",
    "# test_loader = get_test_dataloader_cifar100((0, 0, 0), (1, 1, 1))\n",
    "# acc, _, _ = compute_accuracy(quant_net, test_loader, 'cpu')\n",
    "# accquant, _, _ = compute_accuracy(quant_net, test_loader, 'cpu')\n",
    "# acc20, _, _ = compute_accuracy(pruned_model_2, test_loader, 'cpu')\n",
    "# acc40, _, _ = compute_accuracy(pruned_model_4, test_loader, 'cpu')\n",
    "# acc60, _, _ = compute_accuracy(pruned_model_6, test_loader, 'cpu')\n",
    "# acc80, _, _ = compute_accuracy(pruned_model_8, test_loader, 'cpu')\n",
    "# print(acc, accquant, acc20, acc40, acc60, acc80)\n",
    "# 0.6387 0.6387 0.6395 0.6395 0.6405 0.5619"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-victor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-symbol",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-wright",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-sunrise",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-thermal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-consensus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-booth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-alarm",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-genetics",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-percentage",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5f3f0cb6f7dc525fd91e30599dc917c9059637fcae9cfadc503d008ae5db0235"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
