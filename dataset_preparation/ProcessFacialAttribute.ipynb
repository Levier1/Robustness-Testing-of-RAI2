{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "another-filename",
   "metadata": {},
   "source": [
    "# FairFace statistics\n",
    "* age\n",
    "\n",
    "(array(['0-2', '10-19', '20-29', '3-9', '30-39', '40-49', '50-59', '60-69',\n",
    "       'more than 70'], dtype=object), array([ 1792,  9103, 25598, 10408, 19250, 10744,  6228,  2779,   842]))\n",
    "       \n",
    "(array(['0-2', '10-19', '20-29', '3-9', '30-39', '40-49', '50-59', '60-69',\n",
    "       'more than 70'], dtype=object), array([ 199, 1181, 3300, 1356, 2330, 1353,  796,  321,  118]))\n",
    "       \n",
    "* gender\n",
    "\n",
    "(array(['Female', 'Male'], dtype=object), array([40758, 45986]))\n",
    "\n",
    "(array(['Female', 'Male'], dtype=object), array([5162, 5792]))\n",
    "\n",
    "* race\n",
    "\n",
    "(array(['Black', 'East Asian', 'Indian', 'Latino_Hispanic',\n",
    "       'Middle Eastern', 'Southeast Asian', 'White'], dtype=object), array([12233, 12287, 12319, 13367,  9216, 10795, 16527]))\n",
    "       \n",
    "(array(['Black', 'East Asian', 'Indian', 'Latino_Hispanic',\n",
    "       'Middle Eastern', 'Southeast Asian', 'White'], dtype=object), array([1556, 1550, 1516, 1623, 1209, 1415, 2085]))\n",
    "\n",
    "get statistics via example code:\n",
    "```\n",
    "# print(np.unique(utk_train_label[:, 0], return_counts=True))\n",
    "# print(np.unique(utk_train_label[:, 2], return_counts=True)[0])\n",
    "# print(np.unique(utk_train_label[:, 2], return_counts=True)[1] / len(utk_train_label))\n",
    "\n",
    "# print(np.unique(fair_train_label[:, 0], return_counts=True)[0])\n",
    "\n",
    "# print(np.unique(fair_train_label[:, 2], return_counts=True)[0])\n",
    "# print(np.unique(fair_train_label[:, 2], return_counts=True)[1] / len(fair_train_label))\n",
    "\n",
    "\n",
    "\n",
    "# print(np.unique(utk_train_label[:, 0], return_counts=True)[0])\n",
    "# print(np.unique(utk_train_label[:, 0], return_counts=True)[1] / len(utk_train_label))\n",
    "\n",
    "# print(np.unique(fair_train_label[:, 0], return_counts=True)[0])\n",
    "# print(np.unique(fair_train_label[:, 0], return_counts=True)[1] / len(fair_train_label))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "personal-korea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "funky-hacker",
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_train_img, fair_train_label, fair_val_img, fair_val_label = pickle.load(\n",
    "    open(\"raw/fairface_train_val_pair.pkl\", \"rb\"))\n",
    "fair_train_img, fair_train_label = np.array(fair_train_img), np.array(fair_train_label)\n",
    "fair_val_img, fair_val_label = np.array(fair_val_img), np.array(fair_val_label)\n",
    "utk_train_img, utk_train_label = pickle.load(\n",
    "    open(\"raw/utkface_train_pair.pkl\", \"rb\"))\n",
    "utk_train_img, utk_train_label = np.array(utk_train_img), np.array(utk_train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-serve",
   "metadata": {},
   "source": [
    "# Unify labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aboriginal-automation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_race_label(race):\n",
    "    if 'Asian' in race:\n",
    "        return 'Asian'\n",
    "    elif race == 'Latino_Hispanic':\n",
    "        return 'Others'\n",
    "    elif race == 'Middle Eastern':\n",
    "        return 'Others'\n",
    "    else:\n",
    "        return race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "removable-allen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_age_label(age):\n",
    "    if age in ['0-2', '10-19', '20-29', '3-9']:\n",
    "        return 'Young'\n",
    "    return 'Old'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "legislative-desire",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = map(unify_age_label, fair_train_label[:, 0])\n",
    "fair_train_label[:, 0] = np.array(list(x))\n",
    "x = map(unify_age_label, utk_train_label[:, 0])\n",
    "utk_train_label[:, 0] = np.array(list(x))\n",
    "x = map(unify_age_label, fair_val_label[:, 0])\n",
    "fair_val_label[:, 0] = np.array(list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "august-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = map(unify_race_label, fair_train_label[:, 2])\n",
    "fair_train_label[:, 2] = np.array(list(x))\n",
    "x = map(unify_race_label, fair_val_label[:, 2])\n",
    "fair_val_label[:, 2] = np.array(list(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-thickness",
   "metadata": {},
   "source": [
    "# random select into equal size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "thorough-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2022)\n",
    "permute_idx = np.random.permutation(len(fair_train_img))\n",
    "fair_train_img_set1, fair_train_label_set1 = fair_train_img[permute_idx[:len(utk_train_img)]], fair_train_label[permute_idx[:len(utk_train_img)]]\n",
    "fair_train_img_set2, fair_train_label_set2 = fair_train_img[permute_idx[len(utk_train_img):2 * len(utk_train_img)]], fair_train_label[permute_idx[len(utk_train_img):2 * len(utk_train_img)]]\n",
    "fair_train_img_set_rest, fair_train_label_set_rest = fair_train_img[permute_idx[len(utk_train_img):]], fair_train_label[permute_idx[len(utk_train_img):]]\n",
    "\n",
    "# np.random.seed(2022)\n",
    "# fair_subset_idx = np.random.choice(len(fair_train_img), len(utk_train_img) * 2, replace=False)\n",
    "# fair_train_img_set1, fair_train_label_set1 = fair_train_img[fair_subset_idx[:len(utk_train_img)]], fair_train_label[fair_subset_idx[:len(utk_train_img)]]\n",
    "# fair_train_img_set2, fair_train_label_set2 = fair_train_img[fair_subset_idx[len(utk_train_img):]], fair_train_label[fair_subset_idx[len(utk_train_img):]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-catch",
   "metadata": {},
   "source": [
    "# Resize to 128 * 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "floating-overall",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = 128\n",
    "transform = transforms.Compose([transforms.Resize(im_size), transforms.ToTensor()])\n",
    "# to_pil = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "successful-folks",
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_train_img_set1_tensor = []\n",
    "for x in fair_train_img_set1:\n",
    "    fair_train_img_set1_tensor.append(transform(Image.fromarray(x)))\n",
    "fair_train_img_set1_tensor = torch.stack(fair_train_img_set1_tensor)\n",
    "\n",
    "fair_train_img_set2_tensor = []\n",
    "for x in fair_train_img_set2:\n",
    "    fair_train_img_set2_tensor.append(transform(Image.fromarray(x)))\n",
    "fair_train_img_set2_tensor = torch.stack(fair_train_img_set2_tensor)\n",
    "\n",
    "fair_train_img_set_rest_tensor = []\n",
    "for x in fair_train_img_set_rest:\n",
    "    fair_train_img_set_rest_tensor.append(transform(Image.fromarray(x)))\n",
    "fair_train_img_set_rest_tensor = torch.stack(fair_train_img_set_rest_tensor)\n",
    "\n",
    "fair_val_img_tensor = []\n",
    "for x in fair_val_img:\n",
    "    fair_val_img_tensor.append(transform(Image.fromarray(x)))\n",
    "fair_val_img_tensor = torch.stack(fair_val_img_tensor)\n",
    "    \n",
    "utk_train_img_tensor = []\n",
    "for x in utk_train_img:\n",
    "    utk_train_img_tensor.append(transform(Image.fromarray(x)))\n",
    "utk_train_img_tensor = torch.stack(utk_train_img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fewer-direction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23705, 3, 128, 128])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fair_train_img_set2_tensor.shape, fair_train_img_set_rest_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-coating",
   "metadata": {},
   "source": [
    "# Encoding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "complimentary-check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['Old_Female_Asian', 'Old_Female_Black', 'Old_Female_Indian',\n",
      "       'Old_Female_Others', 'Old_Female_White', 'Old_Male_Asian',\n",
      "       'Old_Male_Black', 'Old_Male_Indian', 'Old_Male_Others',\n",
      "       'Old_Male_White', 'Young_Female_Asian', 'Young_Female_Black',\n",
      "       'Young_Female_Indian', 'Young_Female_Others', 'Young_Female_White',\n",
      "       'Young_Male_Asian', 'Young_Male_Black', 'Young_Male_Indian',\n",
      "       'Young_Male_Others', 'Young_Male_White'], dtype='<U19'), array([ 958,  700,  710, 1135,  921, 1314,  719,  929, 2153, 1378, 2136,\n",
      "        938,  916, 1492, 1222, 1908,  965,  808, 1429,  974]))\n"
     ]
    }
   ],
   "source": [
    "# omit the race\n",
    "fair_train_label_set1 = np.array(list(map('_'.join, fair_train_label_set1)))\n",
    "fair_train_label_set2 = np.array(list(map('_'.join, fair_train_label_set2)))\n",
    "fair_train_label_set_rest = np.array(list(map('_'.join, fair_train_label_set_rest)))\n",
    "fair_val_label = np.array(list(map('_'.join, fair_val_label)))\n",
    "utk_train_label = np.array(list(map('_'.join, utk_train_label)))\n",
    "assert len(np.unique(fair_train_label_set1)) == len(np.unique(fair_train_label_set2)) == len(np.unique(fair_train_label_set_rest)) == len(np.unique(fair_val_label)) == len(np.unique(utk_train_label))\n",
    "print(np.unique(fair_train_label_set1, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "harmful-watershed",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(fair_train_label_set1)\n",
    "print(le.classes_.shape)\n",
    "fair_train_label_set1 = torch.tensor(le.transform(fair_train_label_set1))\n",
    "fair_train_label_set2 = torch.tensor(le.transform(fair_train_label_set2))\n",
    "fair_train_label_set_rest = torch.tensor(le.transform(fair_train_label_set_rest))\n",
    "fair_val_label = torch.tensor(le.transform(fair_val_label))\n",
    "utk_train_label = torch.tensor(le.transform(utk_train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "super-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((fair_train_img_set1_tensor, fair_train_label_set1), open(\"fairface_set1_tensor.pkl\", \"wb\"))\n",
    "pickle.dump((fair_train_img_set2_tensor, fair_train_label_set2), open(\"fairface_set2_tensor.pkl\", \"wb\"))\n",
    "pickle.dump((fair_train_img_set_rest_tensor, fair_train_label_set_rest), open(\"fairface_set_rest_tensor.pkl\", \"wb\"))\n",
    "pickle.dump((fair_val_img_tensor, fair_val_label), open(\"fairface_val_tensor.pkl\", \"wb\"))\n",
    "pickle.dump((utk_train_img_tensor, utk_train_label), open(\"utk_tensor.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-seminar",
   "metadata": {},
   "source": [
    "# Similarity datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-compromise",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "first-vitamin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0 to 23705 from set1, 0 to 0 from set2.\n",
      "Sample 4741 to 23705 from set1, 0 to 4741 from set2.\n",
      "Sample 9482 to 23705 from set1, 0 to 9482 from set2.\n",
      "Sample 14223 to 23705 from set1, 0 to 14223 from set2.\n",
      "Sample 18964 to 23705 from set1, 0 to 18964 from set2.\n",
      "Sample 23705 to 23705 from set1, 0 to 23705 from set2.\n"
     ]
    }
   ],
   "source": [
    "set1_num = len(fair_train_img_set1_tensor)\n",
    "for intersect_proportion in [1.0, 0.8, 0.6, 0.4, 0.2, 0.0]:\n",
    "    shift = int(intersect_proportion * set1_num)\n",
    "    print(f\"Sample {set1_num - shift} to {set1_num} from set1, 0 to {set1_num - shift} from set2.\")\n",
    "    X_tensor = torch.cat([fair_train_img_set1_tensor[set1_num - shift:], fair_train_img_set2_tensor[:set1_num - shift]])\n",
    "    y_tensor = torch.cat([fair_train_label_set1[set1_num - shift:], fair_train_label_set2[:set1_num - shift]])\n",
    "    pickle.dump((X_tensor, y_tensor), open(f\"fairface_similarity/intersect_{intersect_proportion}.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "overall-election",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0 to 23705 from set1, 0 to 0 from set2.\n",
      "Sample 4741 to 23705 from set1, 0 to 4741 from set2.\n",
      "Sample 9482 to 23705 from set1, 0 to 9482 from set2.\n",
      "Sample 14223 to 23705 from set1, 0 to 14223 from set2.\n",
      "Sample 18964 to 23705 from set1, 0 to 18964 from set2.\n",
      "Sample 23705 to 23705 from set1, 0 to 23705 from set2.\n"
     ]
    }
   ],
   "source": [
    "set1_num = len(fair_train_img_set1_tensor)\n",
    "for intersect_proportion in [1.0, 0.8, 0.6, 0.4, 0.2, 0.0]:\n",
    "    shift = int(intersect_proportion * set1_num)\n",
    "    print(f\"Sample {set1_num - shift} to {set1_num} from set1, 0 to {set1_num - shift} from set2.\")\n",
    "    X_tensor = torch.cat([fair_train_img_set1_tensor[set1_num - shift:], utk_train_img_tensor[:set1_num - shift]])\n",
    "    y_tensor = torch.cat([fair_train_label_set1[set1_num - shift:], utk_train_label[:set1_num - shift]])\n",
    "    pickle.dump((X_tensor, y_tensor), open(f\"fairface_utk_mix/intersect_{intersect_proportion}.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-purse",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 ('cv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "0eeec672e8831509c37f92c71492cd90b836b65aedea0b052a189628939cf9ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
